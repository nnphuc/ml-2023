{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52f58ae-371d-440b-9146-470a9644bddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_867/3639738616.py:2: DeprecationWarning: Deprecated since Python 3.4 and slated for removal in Python 3.12; use importlib.util.find_spec() instead\n",
      "  if not importlib.find_loader(\"transformers\"):\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "if not importlib.find_loader(\"transformers\"):\n",
    "    !pip install -q transformers bitsandbytes peft trl accelerate xformers wandb datasets gradio\n",
    "    !pip install -q spicy\n",
    "    !pip install -q torchtext\n",
    "    !pip install -q nltk\n",
    "    !pip install -q tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841540ef-0638-403c-8e88-7f1fed6a6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import wandb\n",
    "import gradio\n",
    "import platform\n",
    "from huggingface_hub import notebook_login\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import tqdm\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d7207e1-c361-44ef-99b3-8056327845ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of CUDA devices: 1\n",
      "--- CUDA Device 0 ---\n",
      "Name: NVIDIA RTX A4000\n",
      "Compute Capability: (8, 6)\n",
      "Total Memory: 16891248640 bytes\n",
      "--- CPU Information ---\n",
      "Processor: x86_64\n",
      "System: Linux 5.4.0-163-generic\n",
      "Python Version: 3.10.13\n"
     ]
    }
   ],
   "source": [
    "def print_system_specs():\n",
    "    # Check if CUDA is available\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    print(\"CUDA Available:\", is_cuda_available)\n",
    "# Get the number of available CUDA devices\n",
    "    num_cuda_devices = torch.cuda.device_count()\n",
    "    print(\"Number of CUDA devices:\", num_cuda_devices)\n",
    "    if is_cuda_available:\n",
    "        for i in range(num_cuda_devices):\n",
    "            # Get CUDA device properties\n",
    "            device = torch.device('cuda', i)\n",
    "            print(f\"--- CUDA Device {i} ---\")\n",
    "            print(\"Name:\", torch.cuda.get_device_name(i))\n",
    "            print(\"Compute Capability:\", torch.cuda.get_device_capability(i))\n",
    "            print(\"Total Memory:\", torch.cuda.get_device_properties(i).total_memory, \"bytes\")\n",
    "    # Get CPU information\n",
    "    print(\"--- CPU Information ---\")\n",
    "    print(\"Processor:\", platform.processor())\n",
    "    print(\"System:\", platform.system(), platform.release())\n",
    "    print(\"Python Version:\", platform.python_version())\n",
    "print_system_specs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e892e417-83b0-42fd-a884-c1700812abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre trained model\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8fee06-3c7f-4d1d-9f29-9ff40c62f857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "import os\n",
    "huggingface_hub.login(\"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\")\n",
    "\n",
    "!mkdir -p /workspace/cache\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/workspace/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf07738d-89df-4cb5-a28d-fe357bbc83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config\n",
    "    \n",
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=16,  # dimension of the updated matrices\n",
    "        lora_alpha=64,  # parameter for scaling\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "006f3cc1-0ae2-4041-b02a-5a75a20341b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{40960}MB'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2f758b-5ec1-4663-b417-c24233e6e6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6bd0b4e28c4d90964d1849b1151f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model from HF with user's token and with bitsandbytes config\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\" \n",
    "\n",
    "bnb_config = create_bnb_config()\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6ad1acf-3655-4b80-a74f-c17c38b051b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563f4775-2c89-43a5-8e90-043526cb3644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all params: 3,540,389,888 || trainable params: 39,976,960 || trainable%: 1.1291682911958425\n"
     ]
    }
   ],
   "source": [
    "# Get lora module names\n",
    "modules = find_all_linear_names(model)\n",
    "\n",
    "# Create PEFT config for these modules and wrap the model to PEFT\n",
    "peft_config = create_peft_config(modules)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print information about the percentage of trainable parameters\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68554081-3587-4326-8363-a47f5dd3f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clear the memory footprint\n",
    "# del model, trainer\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "417f852d-41ea-4085-b5ed-885fe5d4fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MAX_LENGTH = 32\n",
    "DEVICE = 'cuda'\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.model_max_length = MODEL_MAX_LENGTH\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c877131-8302-449b-8393-505963c1db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples, padding='max_length', truncation=True, return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f98ead0d-8399-469f-ab42-a45d9a31785c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 263, 11203, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = preprocess_function(\"a dog\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "722c665f-c5df-4831-8f67-b90e5e7f6fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"what\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "# tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "#assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e4fa6d2-dd33-4aee-b46b-294121ac5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# output = model(input_ids=[inputs[\"input_ids\"]], attention_mask=[inputs[\"attention_mask\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57f55e1f-3ce7-4af8-9ba5-5db27658f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output[\"logits\"].shape,inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "067d178f-74e6-48d1-85bf-2f1fd1572033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mteb/sickr-sts\", split =['test[0:80%]', 'test[-20%:]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64b4261a-8bb8-4979-abda-f718c74d9154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9000, 0.6400, 0.9400,  ..., 0.7400, 0.7600, 0.9200],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_ds(df):\n",
    "    token_ids1 = []\n",
    "    mask1 = []\n",
    "    token_ids2 = []\n",
    "    mask2 = []\n",
    "\n",
    "    score = []\n",
    "    for i in range(len(df)):\n",
    "        r = df.iloc[i,:]\n",
    "        #print(r[\"sentence1\"])\n",
    "        s1 = preprocess_function(r[\"sentence1\"])\n",
    "        token_ids1.append(s1[\"input_ids\"])\n",
    "        mask1.append(s1[\"attention_mask\"])\n",
    "\n",
    "        s2 = preprocess_function(r[\"sentence2\"])\n",
    "        token_ids2.append(s2[\"input_ids\"])\n",
    "        mask2.append(s2[\"attention_mask\"])\n",
    "\n",
    "        score.append(r[\"score\"]/5)\n",
    "        try:\n",
    "            assert( 0 <= r[\"score\"] <= 5)\n",
    "            assert(sum(s1[\"attention_mask\"]) > 1)\n",
    "            assert(sum(s2[\"attention_mask\"]) > 1)\n",
    "        except:\n",
    "            print(r)\n",
    "            break\n",
    "            \n",
    "    return torch.tensor(token_ids1), torch.tensor(mask1), torch.tensor(token_ids2), torch.tensor(mask2), torch.tensor(score)\n",
    "token1, mask1, token2, mask2, score = create_ds(dataset[0].to_pandas())\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2acc33c0-e5d1-48a6-ae3d-27f5e7d50a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "train_ds = TensorDataset(\n",
    "    token1, mask1, token2,mask2, score)\n",
    "\n",
    "test_ds = TensorDataset(*create_ds(dataset[1].to_pandas()))\n",
    "\n",
    "train_dl =  DataLoader(\n",
    "            train_ds,\n",
    "            sampler = RandomSampler(train_ds),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "test_dl = DataLoader(\n",
    "            test_ds,\n",
    "            batch_size = batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "c = 0\n",
    "\n",
    "for b in train_dl:\n",
    "    tmp_i = b[0].to(DEVICE)\n",
    "    tmp_m = b[1].to(DEVICE)\n",
    "    out = model(input_ids=b[0].to(DEVICE), attention_mask=b[1].to(DEVICE), output_hidden_states=False)\n",
    "    # print(b)\n",
    "    # print(out[\"logits\"])\n",
    "    break\n",
    "    if torch.isnan(out[\"logits\"]).any():\n",
    "        c+=1\n",
    "\n",
    "print(c)\n",
    "    \n",
    "#assert False\n",
    "#out[\"hidden_states\"][1].shape, b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e920b2b3-62e6-4af9-b5fa-6fc2bbd6a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight.to(x.device)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        # self.norm = RMSNorm(1)\n",
    "        # self.size = 32000\n",
    "        #self.dense = nn.Linear(self.size, 256)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        #print(input_ids, attention_mask)\n",
    "        output_vectors = []\n",
    "        output = self.model(input_ids=input_ids, attention_mask = attention_mask, output_hidden_states=True)\n",
    "        #print(output)\n",
    "        # hidden_states = output[\"hidden_states\"]\n",
    "        # out = torch.mean(hidden_states[-1], dim=1)\n",
    "        token_embeddings = output[\"logits\"]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        # print(token_embeddings.shape, input_mask_expanded.shape)\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        # print(sum_embeddings.shape)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-3)\n",
    "        # print(sum_mask)\n",
    "        output_vectors.append(sum_embeddings / sum_mask)\n",
    "        \n",
    "        # out = F.relu(out)\n",
    "        # out = self.dense(out) \n",
    "        # out = self.norm(out)\n",
    "        output_vector = torch.cat(output_vectors, 1)\n",
    "        return output_vector\n",
    "my_model = Model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef0026a5-53b9-46bd-90f9-2f94fee702c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_model(tmp_i, tmp_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3203bb84-333f-4c3c-9ad0-078110583e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Iterable, Dict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CosineSimilarityLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, loss_fct = nn.MSELoss(), cos_score_transformation=nn.Identity()):\n",
    "        super(CosineSimilarityLoss, self).__init__()\n",
    "        self.loss_fct = loss_fct\n",
    "        self.cos_score_transformation = cos_score_transformation\n",
    "\n",
    "\n",
    "    def forward(self, embeddings, labels: Tensor):\n",
    "        emb_1 = embeddings[0]\n",
    "        emb_2 = embeddings[1]\n",
    "        output = self.cos_score_transformation(nn.functional.cosine_similarity(emb_1, emb_2))\n",
    "        # print(\"cosin\", output, labels)\n",
    "        return self.loss_fct(output, labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68ae16b9-54fe-4613-88ae-2a0e5ff1ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def test(model, dl, crition, name=\"test\", device=DEVICE):\n",
    "    print(\"test model \", name)\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    # model.eval()\n",
    "    avg_loss = 0\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, b in enumerate(tqdm.tqdm(dl)):\n",
    "            i1, m1, i2, m2, s = b\n",
    "            emb_1 = model(i1.to(device), m1.to(device))\n",
    "            emb_2 = model(i2.to(device), m2.to(device))\n",
    "            score = nn.functional.cosine_similarity(emb_1, emb_2)\n",
    "            s1.append(score.cpu().detach().numpy())\n",
    "            s2.append(s.cpu().detach().numpy())\n",
    "            \n",
    "            # print(i, score, s)\n",
    "            # if i == 12: \n",
    "            #     print(emb_1[1], emb_2[1], i2[1], m2[1])\n",
    "            #     print(torch.sum(emb_1[1] * emb_2[1]))\n",
    "            #     break\n",
    "            avg_loss += crition([emb_1,emb_2], s.to(device))\n",
    "    x = np.hstack(s1)\n",
    "    y = np.hstack(s2)\n",
    "    p = stats.spearmanr(x,y)\n",
    "    print(f\"avg_{name}_loss = {avg_loss/len(dl)}, p= {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3916502-58ca-4512-9cf5-b8507cbe8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crition = CosineSimilarityLoss().to(DEVICE)\n",
    "#test(my_model, train_dl, crition, \"train\")\n",
    "# my_model.to(DEVICE)\n",
    "# test(my_model, test_dl, crition, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10b676-09f7-4b1d-9b2f-ae6f266975cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d68f9fead57434e8b5e19fca8b625ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356fff0df0b8426a9659f201add6627a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1986 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.04558207094669342, epoch=0\n",
      "loss = 0.14968061447143555, epoch=0\n",
      "loss = 0.05962935462594032, epoch=0\n",
      "loss = 0.13186804950237274, epoch=0\n",
      "loss = 0.1361144483089447, epoch=0\n",
      "loss = 0.026338297873735428, epoch=0\n",
      "loss = 0.07838793843984604, epoch=0\n",
      "loss = 0.035246990621089935, epoch=0\n",
      "loss = 0.035578273236751556, epoch=0\n",
      "loss = 0.12680256366729736, epoch=0\n",
      "loss = 0.02703830972313881, epoch=0\n",
      "loss = 0.031304340809583664, epoch=0\n",
      "loss = 0.019219284877181053, epoch=0\n",
      "loss = 0.0689154788851738, epoch=0\n",
      "loss = 0.058868542313575745, epoch=0\n",
      "loss = 0.041068125516176224, epoch=0\n",
      "loss = 0.0190521702170372, epoch=0\n",
      "loss = 0.027981478720903397, epoch=0\n",
      "loss = 0.005916631314903498, epoch=0\n",
      "loss = 0.011881292797625065, epoch=0\n",
      "loss = 0.029838092625141144, epoch=0\n",
      "loss = 0.005967052653431892, epoch=0\n",
      "loss = 0.05730627477169037, epoch=0\n",
      "loss = 0.021394841372966766, epoch=0\n",
      "loss = 0.02057584747672081, epoch=0\n",
      "loss = 0.03176486864686012, epoch=0\n",
      "loss = 0.044117674231529236, epoch=0\n",
      "loss = 0.019326504319906235, epoch=0\n",
      "loss = 0.00853669922798872, epoch=0\n",
      "loss = 0.021294381469488144, epoch=0\n",
      "loss = 0.015062374994158745, epoch=0\n",
      "loss = 0.028853468596935272, epoch=0\n",
      "loss = 0.0012585909571498632, epoch=0\n",
      "loss = 0.00407874071970582, epoch=0\n",
      "loss = 0.028282810002565384, epoch=0\n",
      "loss = 0.006315809674561024, epoch=0\n",
      "loss = 0.021554775536060333, epoch=0\n",
      "loss = 0.019741356372833252, epoch=0\n",
      "loss = 0.015347253531217575, epoch=0\n",
      "loss = 0.007518411614000797, epoch=0\n",
      "loss = 0.012956680729985237, epoch=0\n",
      "loss = 0.013382035307586193, epoch=0\n",
      "loss = 0.010704941116273403, epoch=0\n",
      "loss = 0.002559018088504672, epoch=0\n",
      "loss = 0.00670985970646143, epoch=0\n",
      "loss = 0.035182178020477295, epoch=0\n",
      "loss = 0.0014196077827364206, epoch=0\n",
      "loss = 0.0038793275598436594, epoch=0\n",
      "loss = 0.023229055106639862, epoch=0\n",
      "loss = 0.009048748761415482, epoch=0\n",
      "loss = 0.0017725146608427167, epoch=0\n",
      "loss = 0.041056595742702484, epoch=0\n",
      "loss = 0.00820735190063715, epoch=0\n",
      "loss = 0.00804270152002573, epoch=0\n",
      "loss = 0.005148224998265505, epoch=0\n",
      "loss = 0.005837735719978809, epoch=0\n",
      "loss = 0.014009579084813595, epoch=0\n",
      "loss = 0.0050728414207696915, epoch=0\n",
      "loss = 0.01841614954173565, epoch=0\n",
      "loss = 0.015570392832159996, epoch=0\n",
      "loss = 0.0038971970789134502, epoch=0\n",
      "loss = 0.011846358887851238, epoch=0\n",
      "loss = 0.019171670079231262, epoch=0\n",
      "loss = 0.010336834006011486, epoch=0\n",
      "loss = 0.009273658506572247, epoch=0\n",
      "loss = 0.004261339083313942, epoch=0\n",
      "avg_train_loss = 0.02963510037053308, p= SignificanceResult(statistic=0.5794780047343234, pvalue=0.0)\n",
      "test model  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/497 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/497 [00:00<05:24,  1.53it/s]\u001b[A\n",
      "  0%|          | 2/497 [00:01<04:20,  1.90it/s]\u001b[A\n",
      "  1%|          | 3/497 [00:01<04:00,  2.05it/s]\u001b[A\n",
      "  1%|          | 4/497 [00:01<03:50,  2.14it/s]\u001b[A\n",
      "  1%|          | 5/497 [00:02<03:45,  2.19it/s]\u001b[A\n",
      "  1%|          | 6/497 [00:02<03:40,  2.22it/s]\u001b[A\n",
      "  1%|▏         | 7/497 [00:03<03:38,  2.24it/s]\u001b[A\n",
      "  2%|▏         | 8/497 [00:03<03:36,  2.26it/s]\u001b[A\n",
      "  2%|▏         | 9/497 [00:04<03:35,  2.27it/s]\u001b[A\n",
      "  2%|▏         | 10/497 [00:04<03:34,  2.27it/s]\u001b[A\n",
      "  2%|▏         | 11/497 [00:05<03:33,  2.28it/s]\u001b[A\n",
      "  2%|▏         | 12/497 [00:05<03:32,  2.28it/s]\u001b[A\n",
      "  3%|▎         | 13/497 [00:05<03:32,  2.28it/s]\u001b[A\n",
      "  3%|▎         | 14/497 [00:06<03:31,  2.28it/s]\u001b[A\n",
      "  3%|▎         | 15/497 [00:06<03:30,  2.29it/s]\u001b[A\n",
      "  3%|▎         | 16/497 [00:07<03:30,  2.29it/s]\u001b[A\n",
      "  3%|▎         | 17/497 [00:07<03:29,  2.29it/s]\u001b[A\n",
      "  4%|▎         | 18/497 [00:08<03:29,  2.29it/s]\u001b[A\n",
      "  4%|▍         | 19/497 [00:08<03:28,  2.29it/s]\u001b[A\n",
      "  4%|▍         | 20/497 [00:08<03:28,  2.29it/s]\u001b[A\n",
      "  4%|▍         | 21/497 [00:09<03:28,  2.29it/s]\u001b[A\n",
      "  4%|▍         | 22/497 [00:09<03:27,  2.29it/s]\u001b[A\n",
      "  5%|▍         | 23/497 [00:10<03:27,  2.29it/s]\u001b[A\n",
      "  5%|▍         | 24/497 [00:10<03:26,  2.29it/s]\u001b[A\n",
      "  5%|▌         | 25/497 [00:11<03:26,  2.29it/s]\u001b[A\n",
      "  5%|▌         | 26/497 [00:11<03:25,  2.29it/s]\u001b[A\n",
      "  5%|▌         | 27/497 [00:12<03:25,  2.29it/s]\u001b[A\n",
      "  6%|▌         | 28/497 [00:12<03:25,  2.29it/s]\u001b[A\n",
      "  6%|▌         | 29/497 [00:12<03:24,  2.29it/s]\u001b[A\n",
      "  6%|▌         | 30/497 [00:13<03:24,  2.29it/s]\u001b[A\n",
      "  6%|▌         | 31/497 [00:13<03:23,  2.29it/s]\u001b[A\n",
      "  6%|▋         | 32/497 [00:14<03:23,  2.29it/s]\u001b[A\n",
      "  7%|▋         | 33/497 [00:14<03:22,  2.29it/s]\u001b[A\n",
      "  7%|▋         | 34/497 [00:15<03:22,  2.29it/s]\u001b[A\n",
      "  7%|▋         | 35/497 [00:15<03:21,  2.29it/s]\u001b[A\n",
      "  7%|▋         | 36/497 [00:15<03:21,  2.29it/s]\u001b[A\n",
      "  7%|▋         | 37/497 [00:16<03:20,  2.29it/s]\u001b[A\n",
      "  8%|▊         | 38/497 [00:16<03:20,  2.29it/s]\u001b[A\n",
      "  8%|▊         | 39/497 [00:17<03:20,  2.29it/s]\u001b[A\n",
      "  8%|▊         | 40/497 [00:17<03:19,  2.29it/s]\u001b[A\n",
      "  8%|▊         | 41/497 [00:18<03:19,  2.29it/s]\u001b[A\n",
      "  8%|▊         | 42/497 [00:18<03:18,  2.29it/s]\u001b[A\n",
      "  9%|▊         | 43/497 [00:19<03:18,  2.29it/s]\u001b[A\n",
      "  9%|▉         | 44/497 [00:19<03:17,  2.29it/s]\u001b[A\n",
      "  9%|▉         | 45/497 [00:19<03:17,  2.29it/s]\u001b[A\n",
      "  9%|▉         | 46/497 [00:20<03:17,  2.29it/s]\u001b[A\n",
      "  9%|▉         | 47/497 [00:20<03:16,  2.29it/s]\u001b[A\n",
      " 10%|▉         | 48/497 [00:21<03:16,  2.28it/s]\u001b[A\n",
      " 10%|▉         | 49/497 [00:21<03:16,  2.28it/s]\u001b[A\n",
      " 10%|█         | 50/497 [00:22<03:16,  2.28it/s]\u001b[A\n",
      " 10%|█         | 51/497 [00:22<03:15,  2.28it/s]\u001b[A\n",
      " 10%|█         | 52/497 [00:22<03:15,  2.28it/s]\u001b[A\n",
      " 11%|█         | 53/497 [00:23<03:14,  2.28it/s]\u001b[A\n",
      " 11%|█         | 54/497 [00:23<03:14,  2.28it/s]\u001b[A\n",
      " 11%|█         | 55/497 [00:24<03:13,  2.28it/s]\u001b[A\n",
      " 11%|█▏        | 56/497 [00:24<03:13,  2.28it/s]\u001b[A\n",
      " 11%|█▏        | 57/497 [00:25<03:12,  2.28it/s]\u001b[A\n",
      " 12%|█▏        | 58/497 [00:25<03:12,  2.28it/s]\u001b[A\n",
      " 12%|█▏        | 59/497 [00:26<03:11,  2.28it/s]\u001b[A\n",
      " 12%|█▏        | 60/497 [00:26<03:11,  2.28it/s]\u001b[A\n",
      " 12%|█▏        | 61/497 [00:26<03:10,  2.28it/s]\u001b[A\n",
      " 12%|█▏        | 62/497 [00:27<03:10,  2.29it/s]\u001b[A\n",
      " 13%|█▎        | 63/497 [00:27<03:10,  2.28it/s]\u001b[A\n",
      " 13%|█▎        | 64/497 [00:28<03:09,  2.28it/s]\u001b[A\n",
      " 13%|█▎        | 65/497 [00:28<03:09,  2.28it/s]\u001b[A\n",
      " 13%|█▎        | 66/497 [00:29<03:08,  2.28it/s]\u001b[A\n",
      " 13%|█▎        | 67/497 [00:29<03:08,  2.28it/s]\u001b[A\n",
      " 14%|█▎        | 68/497 [00:29<03:08,  2.28it/s]\u001b[A\n",
      " 14%|█▍        | 69/497 [00:30<03:07,  2.28it/s]\u001b[A\n",
      " 14%|█▍        | 70/497 [00:30<03:07,  2.28it/s]\u001b[A\n",
      " 14%|█▍        | 71/497 [00:31<03:06,  2.28it/s]\u001b[A\n",
      " 14%|█▍        | 72/497 [00:31<03:08,  2.26it/s]\u001b[A\n",
      " 15%|█▍        | 73/497 [00:32<03:07,  2.26it/s]\u001b[A\n",
      " 15%|█▍        | 74/497 [00:32<03:06,  2.27it/s]\u001b[A\n",
      " 15%|█▌        | 75/497 [00:33<03:05,  2.27it/s]\u001b[A\n",
      " 15%|█▌        | 76/497 [00:33<03:04,  2.28it/s]\u001b[A\n",
      " 15%|█▌        | 77/497 [00:33<03:04,  2.28it/s]\u001b[A\n",
      " 16%|█▌        | 78/497 [00:34<03:05,  2.26it/s]\u001b[A\n",
      " 16%|█▌        | 79/497 [00:34<03:06,  2.25it/s]\u001b[A\n",
      " 16%|█▌        | 80/497 [00:35<03:04,  2.26it/s]\u001b[A\n",
      " 16%|█▋        | 81/497 [00:35<03:04,  2.26it/s]\u001b[A\n",
      " 16%|█▋        | 82/497 [00:36<03:03,  2.27it/s]\u001b[A\n",
      " 17%|█▋        | 83/497 [00:36<03:02,  2.27it/s]\u001b[A\n",
      " 17%|█▋        | 84/497 [00:37<03:01,  2.27it/s]\u001b[A\n",
      " 17%|█▋        | 85/497 [00:37<03:01,  2.28it/s]\u001b[A\n",
      " 17%|█▋        | 86/497 [00:37<03:00,  2.28it/s]\u001b[A\n",
      " 18%|█▊        | 87/497 [00:38<02:59,  2.28it/s]\u001b[A\n",
      " 18%|█▊        | 88/497 [00:38<02:59,  2.28it/s]\u001b[A\n",
      " 18%|█▊        | 89/497 [00:39<02:58,  2.28it/s]\u001b[A\n",
      " 18%|█▊        | 90/497 [00:39<02:58,  2.28it/s]\u001b[A\n",
      " 18%|█▊        | 91/497 [00:40<02:57,  2.28it/s]\u001b[A\n",
      " 19%|█▊        | 92/497 [00:40<02:57,  2.28it/s]\u001b[A\n",
      " 19%|█▊        | 93/497 [00:40<02:57,  2.28it/s]\u001b[A\n",
      " 19%|█▉        | 94/497 [00:41<02:56,  2.28it/s]\u001b[A\n",
      " 19%|█▉        | 95/497 [00:41<02:56,  2.28it/s]\u001b[A\n",
      " 19%|█▉        | 96/497 [00:42<02:55,  2.28it/s]\u001b[A\n",
      " 20%|█▉        | 97/497 [00:42<02:56,  2.27it/s]\u001b[A\n",
      " 20%|█▉        | 98/497 [00:43<02:57,  2.25it/s]\u001b[A\n",
      " 20%|█▉        | 99/497 [00:43<02:56,  2.26it/s]\u001b[A\n",
      " 20%|██        | 100/497 [00:44<02:55,  2.27it/s]\u001b[A\n",
      " 20%|██        | 101/497 [00:44<02:54,  2.27it/s]\u001b[A\n",
      " 21%|██        | 102/497 [00:44<02:53,  2.27it/s]\u001b[A\n",
      " 21%|██        | 103/497 [00:45<02:52,  2.28it/s]\u001b[A\n",
      " 21%|██        | 104/497 [00:45<02:52,  2.28it/s]\u001b[A\n",
      " 21%|██        | 105/497 [00:46<02:51,  2.28it/s]\u001b[A\n",
      " 21%|██▏       | 106/497 [00:46<02:51,  2.28it/s]\u001b[A\n",
      " 22%|██▏       | 107/497 [00:47<02:50,  2.28it/s]\u001b[A\n",
      " 22%|██▏       | 108/497 [00:47<02:52,  2.26it/s]\u001b[A\n",
      " 22%|██▏       | 109/497 [00:48<02:51,  2.27it/s]\u001b[A\n",
      " 22%|██▏       | 110/497 [00:48<02:50,  2.27it/s]\u001b[A\n",
      " 22%|██▏       | 111/497 [00:48<02:49,  2.28it/s]\u001b[A\n",
      " 23%|██▎       | 112/497 [00:49<02:49,  2.28it/s]\u001b[A\n",
      " 23%|██▎       | 113/497 [00:49<02:48,  2.28it/s]\u001b[A\n",
      " 23%|██▎       | 114/497 [00:50<02:49,  2.26it/s]\u001b[A\n",
      " 23%|██▎       | 115/497 [00:50<02:48,  2.27it/s]\u001b[A\n",
      " 23%|██▎       | 116/497 [00:51<02:47,  2.27it/s]\u001b[A\n",
      " 24%|██▎       | 117/497 [00:51<02:47,  2.27it/s]\u001b[A\n",
      " 24%|██▎       | 118/497 [00:51<02:46,  2.27it/s]\u001b[A\n",
      " 24%|██▍       | 119/497 [00:52<02:46,  2.27it/s]\u001b[A\n",
      " 24%|██▍       | 120/497 [00:52<02:45,  2.27it/s]\u001b[A\n",
      " 24%|██▍       | 121/497 [00:53<02:45,  2.27it/s]\u001b[A\n",
      " 25%|██▍       | 122/497 [00:53<02:46,  2.26it/s]\u001b[A\n",
      " 25%|██▍       | 123/497 [00:54<02:45,  2.26it/s]\u001b[A\n",
      " 25%|██▍       | 124/497 [00:54<02:44,  2.27it/s]\u001b[A\n",
      " 25%|██▌       | 125/497 [00:55<02:43,  2.27it/s]\u001b[A\n",
      " 25%|██▌       | 126/497 [00:55<02:43,  2.27it/s]\u001b[A\n",
      " 26%|██▌       | 127/497 [00:55<02:42,  2.28it/s]\u001b[A\n",
      " 26%|██▌       | 128/497 [00:56<02:41,  2.28it/s]\u001b[A\n",
      " 26%|██▌       | 129/497 [00:56<02:41,  2.28it/s]\u001b[A\n",
      " 26%|██▌       | 130/497 [00:57<02:40,  2.28it/s]\u001b[A\n",
      " 26%|██▋       | 131/497 [00:57<02:40,  2.28it/s]\u001b[A\n",
      " 27%|██▋       | 132/497 [00:58<02:39,  2.28it/s]\u001b[A\n",
      " 27%|██▋       | 133/497 [00:58<02:39,  2.28it/s]\u001b[A\n",
      " 27%|██▋       | 134/497 [00:59<02:38,  2.28it/s]\u001b[A\n",
      " 27%|██▋       | 135/497 [00:59<02:38,  2.28it/s]\u001b[A\n",
      " 27%|██▋       | 136/497 [00:59<02:38,  2.28it/s]\u001b[A\n",
      " 28%|██▊       | 137/497 [01:00<02:37,  2.28it/s]\u001b[A\n",
      " 28%|██▊       | 138/497 [01:00<02:37,  2.28it/s]\u001b[A\n",
      " 28%|██▊       | 139/497 [01:01<02:36,  2.28it/s]\u001b[A\n",
      " 28%|██▊       | 140/497 [01:01<02:36,  2.28it/s]\u001b[A\n",
      " 28%|██▊       | 141/497 [01:02<02:35,  2.28it/s]\u001b[A\n",
      " 29%|██▊       | 142/497 [01:02<02:35,  2.28it/s]\u001b[A\n",
      " 29%|██▉       | 143/497 [01:02<02:37,  2.25it/s]\u001b[A\n",
      " 29%|██▉       | 144/497 [01:03<02:36,  2.26it/s]\u001b[A\n",
      " 29%|██▉       | 145/497 [01:03<02:35,  2.26it/s]\u001b[A\n",
      " 29%|██▉       | 146/497 [01:04<02:34,  2.27it/s]\u001b[A\n",
      " 30%|██▉       | 147/497 [01:04<02:33,  2.28it/s]\u001b[A\n",
      " 30%|██▉       | 148/497 [01:05<02:33,  2.28it/s]\u001b[A\n",
      " 30%|██▉       | 149/497 [01:05<02:32,  2.28it/s]\u001b[A\n",
      " 30%|███       | 150/497 [01:06<02:32,  2.28it/s]\u001b[A\n",
      " 30%|███       | 151/497 [01:06<02:32,  2.27it/s]\u001b[A\n",
      " 31%|███       | 152/497 [01:06<02:31,  2.27it/s]\u001b[A\n",
      " 31%|███       | 153/497 [01:07<02:31,  2.27it/s]\u001b[A\n",
      " 31%|███       | 154/497 [01:07<02:30,  2.28it/s]\u001b[A\n",
      " 31%|███       | 155/497 [01:08<02:30,  2.28it/s]\u001b[A\n",
      " 31%|███▏      | 156/497 [01:08<02:29,  2.28it/s]\u001b[A\n",
      " 32%|███▏      | 157/497 [01:09<02:29,  2.28it/s]\u001b[A\n",
      " 32%|███▏      | 158/497 [01:09<02:28,  2.28it/s]\u001b[A\n",
      " 32%|███▏      | 159/497 [01:09<02:28,  2.28it/s]\u001b[A\n",
      " 32%|███▏      | 160/497 [01:10<02:27,  2.28it/s]\u001b[A\n",
      " 32%|███▏      | 161/497 [01:10<02:27,  2.28it/s]\u001b[A\n",
      " 33%|███▎      | 162/497 [01:11<02:26,  2.28it/s]\u001b[A\n",
      " 33%|███▎      | 163/497 [01:11<02:26,  2.28it/s]\u001b[A\n",
      " 33%|███▎      | 164/497 [01:12<02:25,  2.28it/s]\u001b[A\n",
      " 33%|███▎      | 165/497 [01:12<02:25,  2.28it/s]\u001b[A\n",
      " 33%|███▎      | 166/497 [01:13<02:25,  2.28it/s]\u001b[A\n",
      " 34%|███▎      | 167/497 [01:13<02:24,  2.28it/s]\u001b[A\n",
      " 34%|███▍      | 168/497 [01:13<02:24,  2.28it/s]\u001b[A\n",
      " 34%|███▍      | 169/497 [01:14<02:23,  2.28it/s]\u001b[A\n",
      " 34%|███▍      | 170/497 [01:14<02:23,  2.28it/s]\u001b[A\n",
      " 34%|███▍      | 171/497 [01:15<02:22,  2.28it/s]\u001b[A\n",
      " 35%|███▍      | 172/497 [01:15<02:22,  2.28it/s]\u001b[A\n",
      " 35%|███▍      | 173/497 [01:16<02:21,  2.28it/s]\u001b[A\n",
      " 35%|███▌      | 174/497 [01:16<02:21,  2.29it/s]\u001b[A\n",
      " 35%|███▌      | 175/497 [01:16<02:20,  2.29it/s]\u001b[A\n",
      " 35%|███▌      | 176/497 [01:17<02:20,  2.29it/s]\u001b[A\n",
      " 36%|███▌      | 177/497 [01:17<02:19,  2.29it/s]\u001b[A\n",
      " 36%|███▌      | 178/497 [01:18<02:19,  2.28it/s]\u001b[A\n",
      " 36%|███▌      | 179/497 [01:18<02:19,  2.28it/s]\u001b[A\n",
      " 36%|███▌      | 180/497 [01:19<02:18,  2.28it/s]\u001b[A\n",
      " 36%|███▋      | 181/497 [01:19<02:18,  2.28it/s]\u001b[A\n",
      " 37%|███▋      | 182/497 [01:20<02:17,  2.28it/s]\u001b[A\n",
      " 37%|███▋      | 183/497 [01:20<02:17,  2.29it/s]\u001b[A\n",
      " 37%|███▋      | 184/497 [01:20<02:16,  2.29it/s]\u001b[A\n",
      " 37%|███▋      | 185/497 [01:21<02:16,  2.29it/s]\u001b[A\n",
      " 37%|███▋      | 186/497 [01:21<02:16,  2.28it/s]\u001b[A\n",
      " 38%|███▊      | 187/497 [01:22<02:15,  2.28it/s]\u001b[A\n",
      " 38%|███▊      | 188/497 [01:22<02:15,  2.28it/s]\u001b[A\n",
      " 38%|███▊      | 189/497 [01:23<02:14,  2.28it/s]\u001b[A\n",
      " 38%|███▊      | 190/497 [01:23<02:14,  2.28it/s]\u001b[A\n",
      " 38%|███▊      | 191/497 [01:24<02:14,  2.28it/s]\u001b[A\n",
      " 39%|███▊      | 192/497 [01:24<02:13,  2.28it/s]\u001b[A\n",
      " 39%|███▉      | 193/497 [01:24<02:13,  2.28it/s]\u001b[A\n",
      " 39%|███▉      | 194/497 [01:25<02:13,  2.26it/s]\u001b[A\n",
      " 39%|███▉      | 195/497 [01:25<02:13,  2.27it/s]\u001b[A\n",
      " 39%|███▉      | 196/497 [01:26<02:12,  2.27it/s]\u001b[A\n",
      " 40%|███▉      | 197/497 [01:26<02:11,  2.28it/s]\u001b[A\n",
      " 40%|███▉      | 198/497 [01:27<02:11,  2.28it/s]\u001b[A\n",
      " 40%|████      | 199/497 [01:27<02:10,  2.28it/s]\u001b[A\n",
      " 40%|████      | 200/497 [01:27<02:10,  2.28it/s]\u001b[A\n",
      " 40%|████      | 201/497 [01:28<02:09,  2.28it/s]\u001b[A\n",
      " 41%|████      | 202/497 [01:28<02:09,  2.28it/s]\u001b[A\n",
      " 41%|████      | 203/497 [01:29<02:09,  2.28it/s]\u001b[A\n",
      " 41%|████      | 204/497 [01:29<02:08,  2.28it/s]\u001b[A\n",
      " 41%|████      | 205/497 [01:30<02:08,  2.28it/s]\u001b[A\n",
      " 41%|████▏     | 206/497 [01:30<02:07,  2.28it/s]\u001b[A\n",
      " 42%|████▏     | 207/497 [01:31<02:07,  2.28it/s]\u001b[A\n",
      " 42%|████▏     | 208/497 [01:31<02:06,  2.28it/s]\u001b[A\n",
      " 42%|████▏     | 209/497 [01:31<02:06,  2.28it/s]\u001b[A\n",
      " 42%|████▏     | 210/497 [01:32<02:05,  2.28it/s]\u001b[A\n",
      " 42%|████▏     | 211/497 [01:32<02:05,  2.28it/s]\u001b[A\n",
      " 43%|████▎     | 212/497 [01:33<02:04,  2.28it/s]\u001b[A\n",
      " 43%|████▎     | 213/497 [01:33<02:04,  2.28it/s]\u001b[A\n",
      " 43%|████▎     | 214/497 [01:34<02:03,  2.28it/s]\u001b[A\n",
      " 43%|████▎     | 215/497 [01:34<02:03,  2.28it/s]\u001b[A\n",
      " 43%|████▎     | 216/497 [01:34<02:02,  2.29it/s]\u001b[A\n",
      " 44%|████▎     | 217/497 [01:35<02:02,  2.29it/s]\u001b[A\n",
      " 44%|████▍     | 218/497 [01:35<02:02,  2.28it/s]\u001b[A\n",
      " 44%|████▍     | 219/497 [01:36<02:01,  2.29it/s]\u001b[A\n",
      " 44%|████▍     | 220/497 [01:36<02:01,  2.28it/s]\u001b[A\n",
      " 44%|████▍     | 221/497 [01:37<02:00,  2.28it/s]\u001b[A\n",
      " 45%|████▍     | 222/497 [01:37<02:00,  2.28it/s]\u001b[A\n",
      " 45%|████▍     | 223/497 [01:38<02:00,  2.28it/s]\u001b[A\n",
      " 45%|████▌     | 224/497 [01:38<01:59,  2.28it/s]\u001b[A\n",
      " 45%|████▌     | 225/497 [01:38<01:59,  2.28it/s]\u001b[A\n",
      " 45%|████▌     | 226/497 [01:39<01:58,  2.28it/s]\u001b[A\n",
      " 46%|████▌     | 227/497 [01:39<01:58,  2.28it/s]\u001b[A\n",
      " 46%|████▌     | 228/497 [01:40<01:57,  2.28it/s]\u001b[A\n",
      " 46%|████▌     | 229/497 [01:40<01:57,  2.28it/s]\u001b[A\n",
      " 46%|████▋     | 230/497 [01:41<01:57,  2.28it/s]\u001b[A\n",
      " 46%|████▋     | 231/497 [01:41<01:56,  2.28it/s]\u001b[A\n",
      " 47%|████▋     | 232/497 [01:41<01:56,  2.28it/s]\u001b[A\n",
      " 47%|████▋     | 233/497 [01:42<01:55,  2.28it/s]\u001b[A\n",
      " 47%|████▋     | 234/497 [01:42<01:55,  2.28it/s]\u001b[A\n",
      " 47%|████▋     | 235/497 [01:43<01:54,  2.28it/s]\u001b[A\n",
      " 47%|████▋     | 236/497 [01:43<01:54,  2.28it/s]\u001b[A\n",
      " 48%|████▊     | 237/497 [01:44<01:53,  2.28it/s]\u001b[A\n",
      " 48%|████▊     | 238/497 [01:44<01:53,  2.28it/s]\u001b[A\n",
      " 48%|████▊     | 239/497 [01:45<01:52,  2.28it/s]\u001b[A\n",
      " 48%|████▊     | 240/497 [01:45<01:52,  2.28it/s]\u001b[A\n",
      " 48%|████▊     | 241/497 [01:45<01:52,  2.28it/s]\u001b[A\n",
      " 49%|████▊     | 242/497 [01:46<01:51,  2.28it/s]\u001b[A\n",
      " 49%|████▉     | 243/497 [01:46<01:51,  2.28it/s]\u001b[A\n",
      " 49%|████▉     | 244/497 [01:47<01:50,  2.28it/s]\u001b[A\n",
      " 49%|████▉     | 245/497 [01:47<01:50,  2.28it/s]\u001b[A\n",
      " 49%|████▉     | 246/497 [01:48<01:49,  2.28it/s]\u001b[A\n",
      " 50%|████▉     | 247/497 [01:48<01:49,  2.28it/s]\u001b[A\n",
      " 50%|████▉     | 248/497 [01:48<01:48,  2.28it/s]\u001b[A\n",
      " 50%|█████     | 249/497 [01:49<01:48,  2.29it/s]\u001b[A\n",
      " 50%|█████     | 250/497 [01:49<01:48,  2.29it/s]\u001b[A\n",
      " 51%|█████     | 251/497 [01:50<01:47,  2.29it/s]\u001b[A\n",
      " 51%|█████     | 252/497 [01:50<01:47,  2.28it/s]\u001b[A\n",
      " 51%|█████     | 253/497 [01:51<01:46,  2.28it/s]\u001b[A\n",
      " 51%|█████     | 254/497 [01:51<01:46,  2.28it/s]\u001b[A\n",
      " 51%|█████▏    | 255/497 [01:52<01:46,  2.28it/s]\u001b[A\n",
      " 52%|█████▏    | 256/497 [01:52<01:45,  2.28it/s]\u001b[A\n",
      " 52%|█████▏    | 257/497 [01:52<01:45,  2.28it/s]\u001b[A\n",
      " 52%|█████▏    | 258/497 [01:53<01:44,  2.28it/s]\u001b[A\n",
      " 52%|█████▏    | 259/497 [01:53<01:44,  2.28it/s]\u001b[A\n",
      " 52%|█████▏    | 260/497 [01:54<01:43,  2.28it/s]\u001b[A\n",
      " 53%|█████▎    | 261/497 [01:54<01:43,  2.28it/s]\u001b[A\n",
      " 53%|█████▎    | 262/497 [01:55<01:42,  2.28it/s]\u001b[A\n",
      " 53%|█████▎    | 263/497 [01:55<01:42,  2.28it/s]\u001b[A\n",
      " 53%|█████▎    | 264/497 [01:56<01:42,  2.28it/s]\u001b[A\n",
      " 53%|█████▎    | 265/497 [01:56<01:41,  2.28it/s]\u001b[A\n",
      " 54%|█████▎    | 266/497 [01:56<01:41,  2.28it/s]\u001b[A\n",
      " 54%|█████▎    | 267/497 [01:57<01:40,  2.28it/s]\u001b[A\n",
      " 54%|█████▍    | 268/497 [01:57<01:40,  2.28it/s]\u001b[A\n",
      " 54%|█████▍    | 269/497 [01:58<01:39,  2.28it/s]\u001b[A\n",
      " 54%|█████▍    | 270/497 [01:58<01:39,  2.28it/s]\u001b[A\n",
      " 55%|█████▍    | 271/497 [01:59<01:38,  2.29it/s]\u001b[A\n",
      " 55%|█████▍    | 272/497 [01:59<01:38,  2.29it/s]\u001b[A\n",
      " 55%|█████▍    | 273/497 [01:59<01:37,  2.29it/s]\u001b[A\n",
      " 55%|█████▌    | 274/497 [02:00<01:37,  2.29it/s]\u001b[A\n",
      " 55%|█████▌    | 275/497 [02:00<01:37,  2.29it/s]\u001b[A\n",
      " 56%|█████▌    | 276/497 [02:01<01:36,  2.29it/s]\u001b[A\n",
      " 56%|█████▌    | 277/497 [02:01<01:36,  2.28it/s]\u001b[A\n",
      " 56%|█████▌    | 278/497 [02:02<01:35,  2.28it/s]\u001b[A\n",
      " 56%|█████▌    | 279/497 [02:02<01:35,  2.29it/s]\u001b[A\n",
      " 56%|█████▋    | 280/497 [02:03<01:34,  2.28it/s]\u001b[A\n",
      " 57%|█████▋    | 281/497 [02:03<01:34,  2.29it/s]\u001b[A\n",
      " 57%|█████▋    | 282/497 [02:03<01:34,  2.29it/s]\u001b[A\n",
      " 57%|█████▋    | 283/497 [02:04<01:33,  2.29it/s]\u001b[A\n",
      " 57%|█████▋    | 284/497 [02:04<01:33,  2.29it/s]\u001b[A\n",
      " 57%|█████▋    | 285/497 [02:05<01:32,  2.28it/s]\u001b[A\n",
      " 58%|█████▊    | 286/497 [02:05<01:32,  2.28it/s]\u001b[A\n",
      " 58%|█████▊    | 287/497 [02:06<01:31,  2.28it/s]\u001b[A\n",
      " 58%|█████▊    | 288/497 [02:06<01:31,  2.28it/s]\u001b[A\n",
      " 58%|█████▊    | 289/497 [02:06<01:31,  2.28it/s]\u001b[A\n",
      " 58%|█████▊    | 290/497 [02:07<01:30,  2.28it/s]\u001b[A\n",
      " 59%|█████▊    | 291/497 [02:07<01:30,  2.28it/s]\u001b[A\n",
      " 59%|█████▉    | 292/497 [02:08<01:29,  2.28it/s]\u001b[A\n",
      " 59%|█████▉    | 293/497 [02:08<01:29,  2.28it/s]\u001b[A\n",
      " 59%|█████▉    | 294/497 [02:09<01:28,  2.28it/s]\u001b[A\n",
      " 59%|█████▉    | 295/497 [02:09<01:28,  2.28it/s]\u001b[A\n",
      " 60%|█████▉    | 296/497 [02:10<01:28,  2.28it/s]\u001b[A\n",
      " 60%|█████▉    | 297/497 [02:10<01:27,  2.28it/s]\u001b[A\n",
      " 60%|█████▉    | 298/497 [02:10<01:27,  2.28it/s]\u001b[A\n",
      " 60%|██████    | 299/497 [02:11<01:26,  2.28it/s]\u001b[A\n",
      " 60%|██████    | 300/497 [02:11<01:26,  2.28it/s]\u001b[A\n",
      " 61%|██████    | 301/497 [02:12<01:25,  2.28it/s]\u001b[A\n",
      " 61%|██████    | 302/497 [02:12<01:25,  2.28it/s]\u001b[A\n",
      " 61%|██████    | 303/497 [02:13<01:24,  2.28it/s]\u001b[A\n",
      " 61%|██████    | 304/497 [02:13<01:24,  2.28it/s]\u001b[A\n",
      " 61%|██████▏   | 305/497 [02:13<01:24,  2.28it/s]\u001b[A\n",
      " 62%|██████▏   | 306/497 [02:14<01:23,  2.28it/s]\u001b[A\n",
      " 62%|██████▏   | 307/497 [02:14<01:23,  2.28it/s]\u001b[A\n",
      " 62%|██████▏   | 308/497 [02:15<01:22,  2.28it/s]\u001b[A\n",
      " 62%|██████▏   | 309/497 [02:15<01:22,  2.28it/s]\u001b[A\n",
      " 62%|██████▏   | 310/497 [02:16<01:21,  2.28it/s]\u001b[A\n",
      " 63%|██████▎   | 311/497 [02:16<01:21,  2.28it/s]\u001b[A\n",
      " 63%|██████▎   | 312/497 [02:17<01:20,  2.29it/s]\u001b[A\n",
      " 63%|██████▎   | 313/497 [02:17<01:20,  2.28it/s]\u001b[A\n",
      " 63%|██████▎   | 314/497 [02:17<01:20,  2.28it/s]\u001b[A\n",
      " 63%|██████▎   | 315/497 [02:18<01:19,  2.28it/s]\u001b[A\n",
      " 64%|██████▎   | 316/497 [02:18<01:19,  2.28it/s]\u001b[A\n",
      " 64%|██████▍   | 317/497 [02:19<01:18,  2.28it/s]\u001b[A\n",
      " 64%|██████▍   | 318/497 [02:19<01:18,  2.28it/s]\u001b[A\n",
      " 64%|██████▍   | 319/497 [02:20<01:18,  2.28it/s]\u001b[A\n",
      " 64%|██████▍   | 320/497 [02:20<01:17,  2.28it/s]\u001b[A\n",
      " 65%|██████▍   | 321/497 [02:20<01:17,  2.28it/s]\u001b[A\n",
      " 65%|██████▍   | 322/497 [02:21<01:16,  2.28it/s]\u001b[A\n",
      " 65%|██████▍   | 323/497 [02:21<01:16,  2.28it/s]\u001b[A\n",
      " 65%|██████▌   | 324/497 [02:22<01:15,  2.28it/s]\u001b[A\n",
      " 65%|██████▌   | 325/497 [02:22<01:15,  2.28it/s]\u001b[A\n",
      " 66%|██████▌   | 326/497 [02:23<01:14,  2.28it/s]\u001b[A\n",
      " 66%|██████▌   | 327/497 [02:23<01:14,  2.28it/s]\u001b[A\n",
      " 66%|██████▌   | 328/497 [02:24<01:14,  2.28it/s]\u001b[A\n",
      " 66%|██████▌   | 329/497 [02:24<01:13,  2.28it/s]\u001b[A\n",
      " 66%|██████▋   | 330/497 [02:24<01:13,  2.28it/s]\u001b[A\n",
      " 67%|██████▋   | 331/497 [02:25<01:12,  2.28it/s]\u001b[A\n",
      " 67%|██████▋   | 332/497 [02:25<01:12,  2.28it/s]\u001b[A\n",
      " 67%|██████▋   | 333/497 [02:26<01:11,  2.28it/s]\u001b[A\n",
      " 67%|██████▋   | 334/497 [02:26<01:11,  2.28it/s]\u001b[A\n",
      " 67%|██████▋   | 335/497 [02:27<01:11,  2.28it/s]\u001b[A\n",
      " 68%|██████▊   | 336/497 [02:27<01:10,  2.28it/s]\u001b[A\n",
      " 68%|██████▊   | 337/497 [02:27<01:10,  2.27it/s]\u001b[A\n",
      " 68%|██████▊   | 338/497 [02:28<01:09,  2.28it/s]\u001b[A\n",
      " 68%|██████▊   | 339/497 [02:28<01:09,  2.28it/s]\u001b[A\n",
      " 68%|██████▊   | 340/497 [02:29<01:08,  2.28it/s]\u001b[A\n",
      " 69%|██████▊   | 341/497 [02:29<01:08,  2.28it/s]\u001b[A\n",
      " 69%|██████▉   | 342/497 [02:30<01:07,  2.28it/s]\u001b[A\n",
      " 69%|██████▉   | 343/497 [02:30<01:07,  2.28it/s]\u001b[A\n",
      " 69%|██████▉   | 344/497 [02:31<01:07,  2.28it/s]\u001b[A\n",
      " 69%|██████▉   | 345/497 [02:31<01:06,  2.28it/s]\u001b[A\n",
      " 70%|██████▉   | 346/497 [02:31<01:06,  2.28it/s]\u001b[A\n",
      " 70%|██████▉   | 347/497 [02:32<01:05,  2.28it/s]\u001b[A\n",
      " 70%|███████   | 348/497 [02:32<01:05,  2.28it/s]\u001b[A\n",
      " 70%|███████   | 349/497 [02:33<01:04,  2.28it/s]\u001b[A\n",
      " 70%|███████   | 350/497 [02:33<01:04,  2.28it/s]\u001b[A\n",
      " 71%|███████   | 351/497 [02:34<01:03,  2.28it/s]\u001b[A\n",
      " 71%|███████   | 352/497 [02:34<01:03,  2.29it/s]\u001b[A\n",
      " 71%|███████   | 353/497 [02:34<01:03,  2.28it/s]\u001b[A\n",
      " 71%|███████   | 354/497 [02:35<01:02,  2.28it/s]\u001b[A\n",
      " 71%|███████▏  | 355/497 [02:35<01:02,  2.28it/s]\u001b[A\n",
      " 72%|███████▏  | 356/497 [02:36<01:01,  2.28it/s]\u001b[A\n",
      " 72%|███████▏  | 357/497 [02:36<01:01,  2.28it/s]\u001b[A\n",
      " 72%|███████▏  | 358/497 [02:37<01:00,  2.28it/s]\u001b[A\n",
      " 72%|███████▏  | 359/497 [02:37<01:01,  2.25it/s]\u001b[A\n",
      " 72%|███████▏  | 360/497 [02:38<01:00,  2.25it/s]\u001b[A\n",
      " 73%|███████▎  | 361/497 [02:38<01:00,  2.26it/s]\u001b[A\n",
      " 73%|███████▎  | 362/497 [02:38<00:59,  2.27it/s]\u001b[A\n",
      " 73%|███████▎  | 363/497 [02:39<00:59,  2.27it/s]\u001b[A\n",
      " 73%|███████▎  | 364/497 [02:39<00:58,  2.27it/s]\u001b[A\n",
      " 73%|███████▎  | 365/497 [02:40<00:57,  2.28it/s]\u001b[A\n",
      " 74%|███████▎  | 366/497 [02:40<00:57,  2.28it/s]\u001b[A\n",
      " 74%|███████▍  | 367/497 [02:41<00:56,  2.28it/s]\u001b[A\n",
      " 74%|███████▍  | 368/497 [02:41<00:56,  2.28it/s]\u001b[A\n",
      " 74%|███████▍  | 369/497 [02:42<00:56,  2.28it/s]\u001b[A\n",
      " 74%|███████▍  | 370/497 [02:42<00:55,  2.28it/s]\u001b[A\n",
      " 75%|███████▍  | 371/497 [02:42<00:55,  2.28it/s]\u001b[A\n",
      " 75%|███████▍  | 372/497 [02:43<00:54,  2.28it/s]\u001b[A\n",
      " 75%|███████▌  | 373/497 [02:43<00:54,  2.28it/s]\u001b[A\n",
      " 75%|███████▌  | 374/497 [02:44<00:53,  2.28it/s]\u001b[A\n",
      " 75%|███████▌  | 375/497 [02:44<00:53,  2.28it/s]\u001b[A\n",
      " 76%|███████▌  | 376/497 [02:45<00:52,  2.28it/s]\u001b[A\n",
      " 76%|███████▌  | 377/497 [02:45<00:52,  2.28it/s]\u001b[A\n",
      " 76%|███████▌  | 378/497 [02:45<00:52,  2.28it/s]\u001b[A\n",
      " 76%|███████▋  | 379/497 [02:46<00:51,  2.28it/s]\u001b[A\n",
      " 76%|███████▋  | 380/497 [02:46<00:51,  2.28it/s]\u001b[A\n",
      " 77%|███████▋  | 381/497 [02:47<00:50,  2.28it/s]\u001b[A\n",
      " 77%|███████▋  | 382/497 [02:47<00:50,  2.28it/s]\u001b[A\n",
      " 77%|███████▋  | 383/497 [02:48<00:50,  2.28it/s]\u001b[A\n",
      " 77%|███████▋  | 384/497 [02:48<00:49,  2.28it/s]\u001b[A\n",
      " 77%|███████▋  | 385/497 [02:49<00:49,  2.28it/s]\u001b[A\n",
      " 78%|███████▊  | 386/497 [02:49<00:48,  2.28it/s]\u001b[A\n",
      " 78%|███████▊  | 387/497 [02:49<00:48,  2.28it/s]\u001b[A\n",
      " 78%|███████▊  | 388/497 [02:50<00:47,  2.28it/s]\u001b[A\n",
      " 78%|███████▊  | 389/497 [02:50<00:47,  2.28it/s]\u001b[A\n",
      " 78%|███████▊  | 390/497 [02:51<00:46,  2.28it/s]\u001b[A\n",
      " 79%|███████▊  | 391/497 [02:51<00:46,  2.28it/s]\u001b[A\n",
      " 79%|███████▉  | 392/497 [02:52<00:46,  2.28it/s]\u001b[A\n",
      " 79%|███████▉  | 393/497 [02:52<00:45,  2.28it/s]\u001b[A\n",
      " 79%|███████▉  | 394/497 [02:52<00:45,  2.28it/s]\u001b[A\n",
      " 79%|███████▉  | 395/497 [02:53<00:44,  2.28it/s]\u001b[A\n",
      " 80%|███████▉  | 396/497 [02:53<00:44,  2.28it/s]\u001b[A\n",
      " 80%|███████▉  | 397/497 [02:54<00:43,  2.28it/s]\u001b[A\n",
      " 80%|████████  | 398/497 [02:54<00:43,  2.28it/s]\u001b[A\n",
      " 80%|████████  | 399/497 [02:55<00:42,  2.28it/s]\u001b[A\n",
      " 80%|████████  | 400/497 [02:55<00:42,  2.29it/s]\u001b[A\n",
      " 81%|████████  | 401/497 [02:56<00:41,  2.29it/s]\u001b[A\n",
      " 81%|████████  | 402/497 [02:56<00:41,  2.29it/s]\u001b[A\n",
      " 81%|████████  | 403/497 [02:56<00:41,  2.29it/s]\u001b[A\n",
      " 81%|████████▏ | 404/497 [02:57<00:40,  2.28it/s]\u001b[A\n",
      " 81%|████████▏ | 405/497 [02:57<00:40,  2.28it/s]\u001b[A\n",
      " 82%|████████▏ | 406/497 [02:58<00:39,  2.29it/s]\u001b[A\n",
      " 82%|████████▏ | 407/497 [02:58<00:39,  2.29it/s]\u001b[A\n",
      " 82%|████████▏ | 408/497 [02:59<00:38,  2.29it/s]\u001b[A\n",
      " 82%|████████▏ | 409/497 [02:59<00:38,  2.27it/s]\u001b[A\n",
      " 82%|████████▏ | 410/497 [02:59<00:38,  2.28it/s]\u001b[A\n",
      " 83%|████████▎ | 411/497 [03:00<00:37,  2.28it/s]\u001b[A\n",
      " 83%|████████▎ | 412/497 [03:00<00:37,  2.28it/s]\u001b[A\n",
      " 83%|████████▎ | 413/497 [03:01<00:36,  2.28it/s]\u001b[A\n",
      " 83%|████████▎ | 414/497 [03:01<00:36,  2.29it/s]\u001b[A\n",
      " 84%|████████▎ | 415/497 [03:02<00:35,  2.29it/s]\u001b[A\n",
      " 84%|████████▎ | 416/497 [03:02<00:35,  2.26it/s]\u001b[A\n",
      " 84%|████████▍ | 417/497 [03:03<00:35,  2.27it/s]\u001b[A\n",
      " 84%|████████▍ | 418/497 [03:03<00:34,  2.27it/s]\u001b[A\n",
      " 84%|████████▍ | 419/497 [03:03<00:34,  2.28it/s]\u001b[A\n",
      " 85%|████████▍ | 420/497 [03:04<00:33,  2.28it/s]\u001b[A\n",
      " 85%|████████▍ | 421/497 [03:04<00:33,  2.28it/s]\u001b[A\n",
      " 85%|████████▍ | 422/497 [03:05<00:32,  2.28it/s]\u001b[A\n",
      " 85%|████████▌ | 423/497 [03:05<00:32,  2.29it/s]\u001b[A\n",
      " 85%|████████▌ | 424/497 [03:06<00:31,  2.29it/s]\u001b[A\n",
      " 86%|████████▌ | 425/497 [03:06<00:31,  2.29it/s]\u001b[A\n",
      " 86%|████████▌ | 426/497 [03:06<00:31,  2.29it/s]\u001b[A\n",
      " 86%|████████▌ | 427/497 [03:07<00:30,  2.29it/s]\u001b[A\n",
      " 86%|████████▌ | 428/497 [03:07<00:30,  2.29it/s]\u001b[A\n",
      " 86%|████████▋ | 429/497 [03:08<00:29,  2.29it/s]\u001b[A\n",
      " 87%|████████▋ | 430/497 [03:08<00:29,  2.29it/s]\u001b[A\n",
      " 87%|████████▋ | 431/497 [03:09<00:28,  2.29it/s]\u001b[A\n",
      " 87%|████████▋ | 432/497 [03:09<00:28,  2.29it/s]\u001b[A\n",
      " 87%|████████▋ | 433/497 [03:10<00:27,  2.29it/s]\u001b[A\n",
      " 87%|████████▋ | 434/497 [03:10<00:27,  2.29it/s]\u001b[A\n",
      " 88%|████████▊ | 435/497 [03:10<00:27,  2.29it/s]\u001b[A\n",
      " 88%|████████▊ | 436/497 [03:11<00:26,  2.29it/s]\u001b[A\n",
      " 88%|████████▊ | 437/497 [03:11<00:26,  2.29it/s]\u001b[A\n",
      " 88%|████████▊ | 438/497 [03:12<00:25,  2.29it/s]\u001b[A\n",
      " 88%|████████▊ | 439/497 [03:12<00:25,  2.29it/s]\u001b[A\n",
      " 89%|████████▊ | 440/497 [03:13<00:24,  2.29it/s]\u001b[A\n",
      " 89%|████████▊ | 441/497 [03:13<00:24,  2.29it/s]\u001b[A\n",
      " 89%|████████▉ | 442/497 [03:13<00:24,  2.28it/s]\u001b[A\n",
      " 89%|████████▉ | 443/497 [03:14<00:23,  2.28it/s]\u001b[A\n",
      " 89%|████████▉ | 444/497 [03:14<00:23,  2.28it/s]\u001b[A\n",
      " 90%|████████▉ | 445/497 [03:15<00:22,  2.28it/s]\u001b[A\n",
      " 90%|████████▉ | 446/497 [03:15<00:22,  2.28it/s]\u001b[A\n",
      " 90%|████████▉ | 447/497 [03:16<00:21,  2.28it/s]\u001b[A\n",
      " 90%|█████████ | 448/497 [03:16<00:21,  2.28it/s]\u001b[A\n",
      " 90%|█████████ | 449/497 [03:17<00:21,  2.28it/s]\u001b[A\n",
      " 91%|█████████ | 450/497 [03:17<00:20,  2.28it/s]\u001b[A\n",
      " 91%|█████████ | 451/497 [03:17<00:20,  2.28it/s]\u001b[A\n",
      " 91%|█████████ | 452/497 [03:18<00:19,  2.28it/s]\u001b[A\n",
      " 91%|█████████ | 453/497 [03:18<00:19,  2.28it/s]\u001b[A\n",
      " 91%|█████████▏| 454/497 [03:19<00:18,  2.28it/s]\u001b[A\n",
      " 92%|█████████▏| 455/497 [03:19<00:18,  2.28it/s]\u001b[A\n",
      " 92%|█████████▏| 456/497 [03:20<00:17,  2.28it/s]\u001b[A\n",
      " 92%|█████████▏| 457/497 [03:20<00:17,  2.28it/s]\u001b[A\n",
      " 92%|█████████▏| 458/497 [03:20<00:17,  2.28it/s]\u001b[A\n",
      " 92%|█████████▏| 459/497 [03:21<00:16,  2.28it/s]\u001b[A\n",
      " 93%|█████████▎| 460/497 [03:21<00:16,  2.28it/s]\u001b[A\n",
      " 93%|█████████▎| 461/497 [03:22<00:15,  2.28it/s]\u001b[A\n",
      " 93%|█████████▎| 462/497 [03:22<00:15,  2.28it/s]\u001b[A\n",
      " 93%|█████████▎| 463/497 [03:23<00:14,  2.28it/s]\u001b[A\n",
      " 93%|█████████▎| 464/497 [03:23<00:14,  2.28it/s]\u001b[A\n",
      " 94%|█████████▎| 465/497 [03:24<00:14,  2.28it/s]\u001b[A\n",
      " 94%|█████████▍| 466/497 [03:24<00:13,  2.28it/s]\u001b[A\n",
      " 94%|█████████▍| 467/497 [03:24<00:13,  2.28it/s]\u001b[A\n",
      " 94%|█████████▍| 468/497 [03:25<00:12,  2.28it/s]\u001b[A\n",
      " 94%|█████████▍| 469/497 [03:25<00:12,  2.28it/s]\u001b[A\n",
      " 95%|█████████▍| 470/497 [03:26<00:11,  2.28it/s]\u001b[A\n",
      " 95%|█████████▍| 471/497 [03:26<00:11,  2.28it/s]\u001b[A\n",
      " 95%|█████████▍| 472/497 [03:27<00:10,  2.28it/s]\u001b[A\n",
      " 95%|█████████▌| 473/497 [03:27<00:10,  2.28it/s]\u001b[A\n",
      " 95%|█████████▌| 474/497 [03:28<00:10,  2.28it/s]\u001b[A\n",
      " 96%|█████████▌| 475/497 [03:28<00:09,  2.28it/s]\u001b[A\n",
      " 96%|█████████▌| 476/497 [03:28<00:09,  2.28it/s]\u001b[A\n",
      " 96%|█████████▌| 477/497 [03:29<00:08,  2.28it/s]\u001b[A\n",
      " 96%|█████████▌| 478/497 [03:29<00:08,  2.28it/s]\u001b[A\n",
      " 96%|█████████▋| 479/497 [03:30<00:07,  2.29it/s]\u001b[A\n",
      " 97%|█████████▋| 480/497 [03:30<00:07,  2.29it/s]\u001b[A\n",
      " 97%|█████████▋| 481/497 [03:31<00:06,  2.29it/s]\u001b[A\n",
      " 97%|█████████▋| 482/497 [03:31<00:06,  2.29it/s]\u001b[A\n",
      " 97%|█████████▋| 483/497 [03:31<00:06,  2.28it/s]\u001b[A\n",
      " 97%|█████████▋| 484/497 [03:32<00:05,  2.28it/s]\u001b[A\n",
      " 98%|█████████▊| 485/497 [03:32<00:05,  2.28it/s]\u001b[A\n",
      " 98%|█████████▊| 486/497 [03:33<00:04,  2.28it/s]\u001b[A\n",
      " 98%|█████████▊| 487/497 [03:33<00:04,  2.28it/s]\u001b[A\n",
      " 98%|█████████▊| 488/497 [03:34<00:03,  2.28it/s]\u001b[A\n",
      " 98%|█████████▊| 489/497 [03:34<00:03,  2.28it/s]\u001b[A\n",
      " 99%|█████████▊| 490/497 [03:35<00:03,  2.28it/s]\u001b[A\n",
      " 99%|█████████▉| 491/497 [03:35<00:02,  2.28it/s]\u001b[A\n",
      " 99%|█████████▉| 492/497 [03:35<00:02,  2.28it/s]\u001b[A\n",
      " 99%|█████████▉| 493/497 [03:36<00:01,  2.28it/s]\u001b[A\n",
      " 99%|█████████▉| 494/497 [03:36<00:01,  2.28it/s]\u001b[A\n",
      "100%|█████████▉| 495/497 [03:37<00:00,  2.28it/s]\u001b[A\n",
      "100%|█████████▉| 496/497 [03:37<00:00,  2.28it/s]\u001b[A\n",
      "100%|██████████| 497/497 [03:38<00:00,  2.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 0.013127049526509586, p= SignificanceResult(statistic=0.749044344749873, pvalue=0.0)\n",
      "---------- 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a03d14aa1843478376415864615d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1986 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.012721598148345947, epoch=1\n",
      "loss = 0.011700375005602837, epoch=1\n",
      "loss = 0.0016851439140737057, epoch=1\n",
      "loss = 0.0029545193538069725, epoch=1\n",
      "loss = 0.00983358733355999, epoch=1\n",
      "loss = 0.0002878860686905682, epoch=1\n",
      "loss = 0.004397058859467506, epoch=1\n",
      "loss = 0.00481312396004796, epoch=1\n",
      "loss = 0.011572515591979027, epoch=1\n",
      "loss = 0.011762572452425957, epoch=1\n",
      "loss = 0.00654109101742506, epoch=1\n",
      "loss = 0.011110235936939716, epoch=1\n",
      "loss = 0.007711351383477449, epoch=1\n",
      "loss = 0.011772963218390942, epoch=1\n",
      "loss = 0.004048801958560944, epoch=1\n",
      "loss = 0.010375848039984703, epoch=1\n",
      "loss = 0.0395967960357666, epoch=1\n",
      "loss = 0.01057453267276287, epoch=1\n",
      "loss = 0.0024210470728576183, epoch=1\n",
      "loss = 0.010520899668335915, epoch=1\n",
      "loss = 0.010270487517118454, epoch=1\n",
      "loss = 0.00717475451529026, epoch=1\n",
      "loss = 0.01168559119105339, epoch=1\n",
      "loss = 0.009450707584619522, epoch=1\n",
      "loss = 0.013807417824864388, epoch=1\n",
      "loss = 0.008085920475423336, epoch=1\n",
      "loss = 0.010274593718349934, epoch=1\n",
      "loss = 0.023131994530558586, epoch=1\n",
      "loss = 0.011778993532061577, epoch=1\n",
      "loss = 0.0030852393247187138, epoch=1\n",
      "loss = 0.017618000507354736, epoch=1\n",
      "loss = 0.01371406763792038, epoch=1\n",
      "loss = 0.0015922849997878075, epoch=1\n",
      "loss = 0.0014634784311056137, epoch=1\n",
      "loss = 0.004592344164848328, epoch=1\n",
      "loss = 0.009679511189460754, epoch=1\n",
      "loss = 0.020466649904847145, epoch=1\n",
      "loss = 0.0126041816547513, epoch=1\n",
      "loss = 0.005524302367120981, epoch=1\n",
      "loss = 0.010821094736456871, epoch=1\n",
      "loss = 0.032867249101400375, epoch=1\n",
      "loss = 0.005571472458541393, epoch=1\n",
      "loss = 0.020016105845570564, epoch=1\n",
      "loss = 0.020859863609075546, epoch=1\n",
      "loss = 0.028323883190751076, epoch=1\n",
      "loss = 0.00467816973105073, epoch=1\n",
      "loss = 0.018190892413258553, epoch=1\n",
      "loss = 0.0004196201916784048, epoch=1\n",
      "loss = 0.0029002795927226543, epoch=1\n",
      "loss = 0.024328388273715973, epoch=1\n",
      "loss = 0.01071549765765667, epoch=1\n",
      "loss = 0.017274081707000732, epoch=1\n",
      "loss = 0.00398959219455719, epoch=1\n",
      "loss = 0.005870949011296034, epoch=1\n",
      "loss = 0.04472072422504425, epoch=1\n",
      "loss = 0.014584401622414589, epoch=1\n",
      "loss = 0.020404154434800148, epoch=1\n",
      "loss = 0.016089793294668198, epoch=1\n",
      "loss = 0.032963357865810394, epoch=1\n",
      "loss = 0.005299659445881844, epoch=1\n",
      "loss = 0.01698017306625843, epoch=1\n",
      "loss = 0.024546965956687927, epoch=1\n",
      "loss = 0.0180857814848423, epoch=1\n",
      "loss = 0.010507635772228241, epoch=1\n",
      "loss = 0.008927145972847939, epoch=1\n",
      "loss = 0.005457208026200533, epoch=1\n",
      "avg_train_loss = 0.011044314616233859, p= SignificanceResult(statistic=0.8065051360579969, pvalue=0.0)\n",
      "test model  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/497 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/497 [00:00<05:24,  1.53it/s]\u001b[A\n",
      "  0%|          | 2/497 [00:01<04:27,  1.85it/s]\u001b[A\n",
      "  1%|          | 3/497 [00:01<04:08,  1.99it/s]\u001b[A\n",
      "  1%|          | 4/497 [00:02<03:59,  2.06it/s]\u001b[A\n",
      "  1%|          | 5/497 [00:02<03:53,  2.11it/s]\u001b[A\n",
      "  1%|          | 6/497 [00:02<03:49,  2.14it/s]\u001b[A\n",
      "  1%|▏         | 7/497 [00:03<03:47,  2.16it/s]\u001b[A\n",
      "  2%|▏         | 8/497 [00:03<03:46,  2.16it/s]\u001b[A\n",
      "  2%|▏         | 9/497 [00:04<03:44,  2.17it/s]\u001b[A\n",
      "  2%|▏         | 10/497 [00:04<03:43,  2.18it/s]\u001b[A\n",
      "  2%|▏         | 11/497 [00:05<03:43,  2.17it/s]\u001b[A\n",
      "  2%|▏         | 12/497 [00:05<03:42,  2.18it/s]\u001b[A\n",
      "  3%|▎         | 13/497 [00:06<03:42,  2.18it/s]\u001b[A\n",
      "  3%|▎         | 14/497 [00:06<03:41,  2.19it/s]\u001b[A\n",
      "  3%|▎         | 15/497 [00:07<03:40,  2.19it/s]\u001b[A\n",
      "  3%|▎         | 16/497 [00:07<03:40,  2.18it/s]\u001b[A\n",
      "  3%|▎         | 17/497 [00:07<03:39,  2.19it/s]\u001b[A\n",
      "  4%|▎         | 18/497 [00:08<03:39,  2.19it/s]\u001b[A\n",
      "  4%|▍         | 19/497 [00:08<03:39,  2.18it/s]\u001b[A\n",
      "  4%|▍         | 20/497 [00:09<03:38,  2.19it/s]\u001b[A\n",
      "  4%|▍         | 21/497 [00:09<03:37,  2.19it/s]\u001b[A\n",
      "  4%|▍         | 22/497 [00:10<03:37,  2.19it/s]\u001b[A\n",
      "  5%|▍         | 23/497 [00:10<03:36,  2.19it/s]\u001b[A\n",
      "  5%|▍         | 24/497 [00:11<03:37,  2.18it/s]\u001b[A\n",
      "  5%|▌         | 25/497 [00:11<03:37,  2.17it/s]\u001b[A\n",
      "  5%|▌         | 26/497 [00:12<03:36,  2.18it/s]\u001b[A\n",
      "  5%|▌         | 27/497 [00:12<03:36,  2.18it/s]\u001b[A\n",
      "  6%|▌         | 28/497 [00:13<03:35,  2.18it/s]\u001b[A\n",
      "  6%|▌         | 29/497 [00:13<03:35,  2.17it/s]\u001b[A\n",
      "  6%|▌         | 30/497 [00:13<03:34,  2.18it/s]\u001b[A\n",
      "  6%|▌         | 31/497 [00:14<03:33,  2.18it/s]\u001b[A\n",
      "  6%|▋         | 32/497 [00:14<03:33,  2.17it/s]\u001b[A\n",
      "  7%|▋         | 33/497 [00:15<03:33,  2.17it/s]\u001b[A\n",
      "  7%|▋         | 34/497 [00:15<03:32,  2.17it/s]\u001b[A\n",
      "  7%|▋         | 35/497 [00:16<03:31,  2.18it/s]\u001b[A\n",
      "  7%|▋         | 36/497 [00:16<03:31,  2.18it/s]\u001b[A\n",
      "  7%|▋         | 37/497 [00:17<03:30,  2.18it/s]\u001b[A\n",
      "  8%|▊         | 38/497 [00:17<03:30,  2.18it/s]\u001b[A\n",
      "  8%|▊         | 39/497 [00:18<03:30,  2.18it/s]\u001b[A\n",
      "  8%|▊         | 40/497 [00:18<03:29,  2.18it/s]\u001b[A\n",
      "  8%|▊         | 41/497 [00:18<03:29,  2.18it/s]\u001b[A\n",
      "  8%|▊         | 42/497 [00:19<03:28,  2.18it/s]\u001b[A\n",
      "  9%|▊         | 43/497 [00:19<03:27,  2.19it/s]\u001b[A\n",
      "  9%|▉         | 44/497 [00:20<03:27,  2.18it/s]\u001b[A\n",
      "  9%|▉         | 45/497 [00:20<03:27,  2.18it/s]\u001b[A\n",
      "  9%|▉         | 46/497 [00:21<03:26,  2.18it/s]\u001b[A\n",
      "  9%|▉         | 47/497 [00:21<03:25,  2.18it/s]\u001b[A\n",
      " 10%|▉         | 48/497 [00:22<03:25,  2.18it/s]\u001b[A\n",
      " 10%|▉         | 49/497 [00:22<03:26,  2.17it/s]\u001b[A\n",
      " 10%|█         | 50/497 [00:23<03:25,  2.17it/s]\u001b[A\n",
      " 10%|█         | 51/497 [00:23<03:24,  2.18it/s]\u001b[A\n",
      " 10%|█         | 52/497 [00:24<03:23,  2.18it/s]\u001b[A\n",
      " 11%|█         | 53/497 [00:24<03:23,  2.18it/s]\u001b[A\n",
      " 11%|█         | 54/497 [00:24<03:23,  2.18it/s]\u001b[A\n",
      " 11%|█         | 55/497 [00:25<03:22,  2.18it/s]\u001b[A\n",
      " 11%|█▏        | 56/497 [00:25<03:21,  2.18it/s]\u001b[A\n",
      " 11%|█▏        | 57/497 [00:26<03:21,  2.19it/s]\u001b[A\n",
      " 12%|█▏        | 58/497 [00:26<03:21,  2.18it/s]\u001b[A\n",
      " 12%|█▏        | 59/497 [00:27<03:21,  2.18it/s]\u001b[A\n",
      " 12%|█▏        | 60/497 [00:27<03:20,  2.17it/s]\u001b[A\n",
      " 12%|█▏        | 61/497 [00:28<03:20,  2.17it/s]\u001b[A\n",
      " 12%|█▏        | 62/497 [00:28<03:19,  2.18it/s]\u001b[A\n",
      " 13%|█▎        | 63/497 [00:29<03:19,  2.18it/s]\u001b[A\n",
      " 13%|█▎        | 64/497 [00:29<03:18,  2.18it/s]\u001b[A\n",
      " 13%|█▎        | 65/497 [00:30<03:18,  2.17it/s]\u001b[A\n",
      " 13%|█▎        | 66/497 [00:30<03:18,  2.17it/s]\u001b[A\n",
      " 13%|█▎        | 67/497 [00:30<03:17,  2.18it/s]\u001b[A\n",
      " 14%|█▎        | 68/497 [00:31<03:16,  2.19it/s]\u001b[A\n",
      " 14%|█▍        | 69/497 [00:31<03:16,  2.18it/s]\u001b[A\n",
      " 14%|█▍        | 70/497 [00:32<03:16,  2.18it/s]\u001b[A\n",
      " 14%|█▍        | 71/497 [00:32<03:16,  2.17it/s]\u001b[A\n",
      " 14%|█▍        | 72/497 [00:33<03:15,  2.18it/s]\u001b[A\n",
      " 15%|█▍        | 73/497 [00:33<03:14,  2.18it/s]\u001b[A\n",
      " 15%|█▍        | 74/497 [00:34<03:14,  2.17it/s]\u001b[A\n",
      " 15%|█▌        | 75/497 [00:34<03:14,  2.17it/s]\u001b[A\n",
      " 15%|█▌        | 76/497 [00:35<03:13,  2.18it/s]\u001b[A\n",
      " 15%|█▌        | 77/497 [00:35<03:12,  2.18it/s]\u001b[A\n",
      " 16%|█▌        | 78/497 [00:35<03:12,  2.18it/s]\u001b[A\n",
      " 16%|█▌        | 79/497 [00:36<03:12,  2.18it/s]\u001b[A\n",
      " 16%|█▌        | 80/497 [00:36<03:11,  2.17it/s]\u001b[A\n",
      " 16%|█▋        | 81/497 [00:37<03:11,  2.17it/s]\u001b[A\n",
      " 16%|█▋        | 82/497 [00:37<03:10,  2.17it/s]\u001b[A\n",
      " 17%|█▋        | 83/497 [00:38<03:10,  2.17it/s]\u001b[A\n",
      " 17%|█▋        | 84/497 [00:38<03:09,  2.18it/s]\u001b[A\n",
      " 17%|█▋        | 85/497 [00:39<03:09,  2.17it/s]\u001b[A\n",
      " 17%|█▋        | 86/497 [00:39<03:09,  2.17it/s]\u001b[A\n",
      " 18%|█▊        | 87/497 [00:40<03:08,  2.17it/s]\u001b[A\n",
      " 18%|█▊        | 88/497 [00:40<03:07,  2.18it/s]\u001b[A\n",
      " 18%|█▊        | 89/497 [00:41<03:07,  2.18it/s]\u001b[A\n",
      " 18%|█▊        | 90/497 [00:41<03:07,  2.17it/s]\u001b[A\n",
      " 18%|█▊        | 91/497 [00:41<03:07,  2.17it/s]\u001b[A\n",
      " 19%|█▊        | 92/497 [00:42<03:06,  2.17it/s]\u001b[A\n",
      " 19%|█▊        | 93/497 [00:42<03:05,  2.18it/s]\u001b[A\n",
      " 19%|█▉        | 94/497 [00:43<03:08,  2.14it/s]\u001b[A\n",
      " 19%|█▉        | 95/497 [00:43<03:09,  2.13it/s]\u001b[A\n",
      " 19%|█▉        | 96/497 [00:44<03:08,  2.12it/s]\u001b[A\n",
      " 20%|█▉        | 97/497 [00:44<03:07,  2.14it/s]\u001b[A\n",
      " 20%|█▉        | 98/497 [00:45<03:05,  2.15it/s]\u001b[A\n",
      " 20%|█▉        | 99/497 [00:45<03:04,  2.16it/s]\u001b[A\n",
      " 20%|██        | 100/497 [00:46<03:03,  2.16it/s]\u001b[A\n",
      " 20%|██        | 101/497 [00:46<03:03,  2.16it/s]\u001b[A\n",
      " 21%|██        | 102/497 [00:47<03:02,  2.17it/s]\u001b[A\n",
      " 21%|██        | 103/497 [00:47<03:01,  2.18it/s]\u001b[A\n",
      " 21%|██        | 104/497 [00:47<03:00,  2.17it/s]\u001b[A\n",
      " 21%|██        | 105/497 [00:48<03:00,  2.17it/s]\u001b[A\n",
      " 21%|██▏       | 106/497 [00:48<03:00,  2.17it/s]\u001b[A\n",
      " 22%|██▏       | 107/497 [00:49<02:59,  2.17it/s]\u001b[A\n",
      " 22%|██▏       | 108/497 [00:49<02:58,  2.18it/s]\u001b[A\n",
      " 22%|██▏       | 109/497 [00:50<02:57,  2.18it/s]\u001b[A\n",
      " 22%|██▏       | 110/497 [00:50<02:57,  2.18it/s]\u001b[A\n",
      " 22%|██▏       | 111/497 [00:51<02:57,  2.17it/s]\u001b[A\n",
      " 23%|██▎       | 112/497 [00:51<02:56,  2.18it/s]\u001b[A\n",
      " 23%|██▎       | 113/497 [00:52<02:56,  2.17it/s]\u001b[A\n",
      " 23%|██▎       | 114/497 [00:52<02:56,  2.18it/s]\u001b[A\n",
      " 23%|██▎       | 115/497 [00:53<02:55,  2.17it/s]\u001b[A\n",
      " 23%|██▎       | 116/497 [00:53<02:55,  2.17it/s]\u001b[A\n",
      " 24%|██▎       | 117/497 [00:53<02:55,  2.17it/s]\u001b[A\n",
      " 24%|██▎       | 118/497 [00:54<02:54,  2.17it/s]\u001b[A\n",
      " 24%|██▍       | 119/497 [00:54<02:54,  2.17it/s]\u001b[A\n",
      " 24%|██▍       | 120/497 [00:55<02:53,  2.18it/s]\u001b[A\n",
      " 24%|██▍       | 121/497 [00:55<02:53,  2.17it/s]\u001b[A\n",
      " 25%|██▍       | 122/497 [00:56<02:52,  2.17it/s]\u001b[A\n",
      " 25%|██▍       | 123/497 [00:56<02:52,  2.17it/s]\u001b[A\n",
      " 25%|██▍       | 124/497 [00:57<02:51,  2.18it/s]\u001b[A\n",
      " 25%|██▌       | 125/497 [00:57<02:50,  2.18it/s]\u001b[A\n",
      " 25%|██▌       | 126/497 [00:58<02:50,  2.17it/s]\u001b[A\n",
      " 26%|██▌       | 127/497 [00:58<02:50,  2.17it/s]\u001b[A\n",
      " 26%|██▌       | 128/497 [00:59<02:49,  2.18it/s]\u001b[A\n",
      " 26%|██▌       | 129/497 [00:59<02:49,  2.18it/s]\u001b[A\n",
      " 26%|██▌       | 130/497 [00:59<02:48,  2.17it/s]\u001b[A\n",
      " 26%|██▋       | 131/497 [01:00<02:48,  2.17it/s]\u001b[A\n",
      " 27%|██▋       | 132/497 [01:00<02:47,  2.17it/s]\u001b[A\n",
      " 27%|██▋       | 133/497 [01:01<02:47,  2.17it/s]\u001b[A\n",
      " 27%|██▋       | 134/497 [01:01<02:48,  2.16it/s]\u001b[A\n",
      " 27%|██▋       | 135/497 [01:02<02:47,  2.17it/s]\u001b[A\n",
      " 27%|██▋       | 136/497 [01:02<02:46,  2.17it/s]\u001b[A\n",
      " 28%|██▊       | 137/497 [01:03<02:46,  2.16it/s]\u001b[A\n",
      " 28%|██▊       | 138/497 [01:03<02:45,  2.17it/s]\u001b[A\n",
      " 28%|██▊       | 139/497 [01:04<02:44,  2.17it/s]\u001b[A\n",
      " 28%|██▊       | 140/497 [01:04<02:44,  2.18it/s]\u001b[A\n",
      " 28%|██▊       | 141/497 [01:05<02:43,  2.17it/s]\u001b[A\n",
      " 29%|██▊       | 142/497 [01:05<02:43,  2.17it/s]\u001b[A\n",
      " 29%|██▉       | 143/497 [01:05<02:43,  2.17it/s]\u001b[A\n",
      " 29%|██▉       | 144/497 [01:06<02:42,  2.17it/s]\u001b[A\n",
      " 29%|██▉       | 145/497 [01:06<02:42,  2.17it/s]\u001b[A\n",
      " 29%|██▉       | 146/497 [01:07<02:41,  2.17it/s]\u001b[A\n",
      " 30%|██▉       | 147/497 [01:07<02:41,  2.16it/s]\u001b[A\n",
      " 30%|██▉       | 148/497 [01:08<02:40,  2.17it/s]\u001b[A\n",
      " 30%|██▉       | 149/497 [01:08<02:40,  2.17it/s]\u001b[A\n",
      " 30%|███       | 150/497 [01:09<02:39,  2.17it/s]\u001b[A\n",
      " 30%|███       | 151/497 [01:09<02:39,  2.17it/s]\u001b[A\n",
      " 31%|███       | 152/497 [01:10<02:38,  2.17it/s]\u001b[A\n",
      " 31%|███       | 153/497 [01:10<02:38,  2.17it/s]\u001b[A\n",
      " 31%|███       | 154/497 [01:11<02:38,  2.17it/s]\u001b[A\n",
      " 31%|███       | 155/497 [01:11<02:37,  2.17it/s]\u001b[A\n",
      " 31%|███▏      | 156/497 [01:11<02:36,  2.18it/s]\u001b[A\n",
      " 32%|███▏      | 157/497 [01:12<02:36,  2.17it/s]\u001b[A\n",
      " 32%|███▏      | 158/497 [01:12<02:36,  2.17it/s]\u001b[A\n",
      " 32%|███▏      | 159/497 [01:13<02:35,  2.17it/s]\u001b[A\n",
      " 32%|███▏      | 160/497 [01:13<02:34,  2.18it/s]\u001b[A\n",
      " 32%|███▏      | 161/497 [01:14<02:34,  2.17it/s]\u001b[A\n",
      " 33%|███▎      | 162/497 [01:14<02:34,  2.17it/s]\u001b[A\n",
      " 33%|███▎      | 163/497 [01:15<02:34,  2.17it/s]\u001b[A\n",
      " 33%|███▎      | 164/497 [01:15<02:32,  2.18it/s]\u001b[A\n",
      " 33%|███▎      | 165/497 [01:16<02:32,  2.18it/s]\u001b[A\n",
      " 33%|███▎      | 166/497 [01:16<02:32,  2.18it/s]\u001b[A\n",
      " 34%|███▎      | 167/497 [01:16<02:31,  2.17it/s]\u001b[A\n",
      " 34%|███▍      | 168/497 [01:17<02:31,  2.17it/s]\u001b[A\n",
      " 34%|███▍      | 169/497 [01:17<02:30,  2.17it/s]\u001b[A\n",
      " 34%|███▍      | 170/497 [01:18<02:30,  2.17it/s]\u001b[A\n",
      " 34%|███▍      | 171/497 [01:18<02:29,  2.18it/s]\u001b[A\n",
      " 35%|███▍      | 172/497 [01:19<02:29,  2.17it/s]\u001b[A\n",
      " 35%|███▍      | 173/497 [01:19<02:29,  2.17it/s]\u001b[A\n",
      " 35%|███▌      | 174/497 [01:20<02:28,  2.17it/s]\u001b[A\n",
      " 35%|███▌      | 175/497 [01:20<02:28,  2.17it/s]\u001b[A\n",
      " 35%|███▌      | 176/497 [01:21<02:27,  2.18it/s]\u001b[A\n",
      " 36%|███▌      | 177/497 [01:21<02:27,  2.17it/s]\u001b[A\n",
      " 36%|███▌      | 178/497 [01:22<02:27,  2.17it/s]\u001b[A\n",
      " 36%|███▌      | 179/497 [01:22<02:26,  2.17it/s]\u001b[A\n",
      " 36%|███▌      | 180/497 [01:22<02:25,  2.17it/s]\u001b[A\n",
      " 36%|███▋      | 181/497 [01:23<02:25,  2.17it/s]\u001b[A\n",
      " 37%|███▋      | 182/497 [01:23<02:25,  2.17it/s]\u001b[A\n",
      " 37%|███▋      | 183/497 [01:24<02:24,  2.17it/s]\u001b[A\n",
      " 37%|███▋      | 184/497 [01:24<02:23,  2.18it/s]\u001b[A\n",
      " 37%|███▋      | 185/497 [01:25<02:23,  2.18it/s]\u001b[A\n",
      " 37%|███▋      | 186/497 [01:25<02:22,  2.17it/s]\u001b[A\n",
      " 38%|███▊      | 187/497 [01:26<02:22,  2.17it/s]\u001b[A\n",
      " 38%|███▊      | 188/497 [01:26<02:22,  2.17it/s]\u001b[A\n",
      " 38%|███▊      | 189/497 [01:27<02:21,  2.17it/s]\u001b[A\n",
      " 38%|███▊      | 190/497 [01:27<02:21,  2.17it/s]\u001b[A\n",
      " 38%|███▊      | 191/497 [01:28<02:20,  2.17it/s]\u001b[A\n",
      " 39%|███▊      | 192/497 [01:28<02:20,  2.17it/s]\u001b[A\n",
      " 39%|███▉      | 193/497 [01:28<02:19,  2.17it/s]\u001b[A\n",
      " 39%|███▉      | 194/497 [01:29<02:19,  2.17it/s]\u001b[A\n",
      " 39%|███▉      | 195/497 [01:29<02:19,  2.17it/s]\u001b[A\n",
      " 39%|███▉      | 196/497 [01:30<02:18,  2.18it/s]\u001b[A\n",
      " 40%|███▉      | 197/497 [01:30<02:18,  2.16it/s]\u001b[A\n",
      " 40%|███▉      | 198/497 [01:31<02:18,  2.17it/s]\u001b[A\n",
      " 40%|████      | 199/497 [01:31<02:17,  2.17it/s]\u001b[A\n",
      " 40%|████      | 200/497 [01:32<02:16,  2.17it/s]\u001b[A\n",
      " 40%|████      | 201/497 [01:32<02:16,  2.17it/s]\u001b[A\n",
      " 41%|████      | 202/497 [01:33<02:15,  2.18it/s]\u001b[A\n",
      " 41%|████      | 203/497 [01:33<02:15,  2.17it/s]\u001b[A\n",
      " 41%|████      | 204/497 [01:34<02:14,  2.18it/s]\u001b[A\n",
      " 41%|████      | 205/497 [01:34<02:14,  2.18it/s]\u001b[A\n",
      " 41%|████▏     | 206/497 [01:34<02:13,  2.18it/s]\u001b[A\n",
      " 42%|████▏     | 207/497 [01:35<02:13,  2.18it/s]\u001b[A\n",
      " 42%|████▏     | 208/497 [01:35<02:12,  2.18it/s]\u001b[A\n",
      " 42%|████▏     | 209/497 [01:36<02:12,  2.17it/s]\u001b[A\n",
      " 42%|████▏     | 210/497 [01:36<02:11,  2.18it/s]\u001b[A\n",
      " 42%|████▏     | 211/497 [01:37<02:11,  2.18it/s]\u001b[A\n",
      " 43%|████▎     | 212/497 [01:37<02:10,  2.18it/s]\u001b[A\n",
      " 43%|████▎     | 213/497 [01:38<02:10,  2.18it/s]\u001b[A\n",
      " 43%|████▎     | 214/497 [01:38<02:10,  2.18it/s]\u001b[A\n",
      " 43%|████▎     | 215/497 [01:39<02:09,  2.18it/s]\u001b[A\n",
      " 43%|████▎     | 216/497 [01:39<02:08,  2.18it/s]\u001b[A\n",
      " 44%|████▎     | 217/497 [01:39<02:08,  2.18it/s]\u001b[A\n",
      " 44%|████▍     | 218/497 [01:40<02:07,  2.18it/s]\u001b[A\n",
      " 44%|████▍     | 219/497 [01:40<02:07,  2.17it/s]\u001b[A\n",
      " 44%|████▍     | 220/497 [01:41<02:07,  2.18it/s]\u001b[A\n",
      " 44%|████▍     | 221/497 [01:41<02:07,  2.17it/s]\u001b[A\n",
      " 45%|████▍     | 222/497 [01:42<02:06,  2.17it/s]\u001b[A\n",
      " 45%|████▍     | 223/497 [01:42<02:06,  2.17it/s]\u001b[A\n",
      " 45%|████▌     | 224/497 [01:43<02:05,  2.17it/s]\u001b[A\n",
      " 45%|████▌     | 225/497 [01:43<02:04,  2.18it/s]\u001b[A\n",
      " 45%|████▌     | 226/497 [01:44<02:04,  2.18it/s]\u001b[A\n",
      " 46%|████▌     | 227/497 [01:44<02:04,  2.16it/s]\u001b[A\n",
      " 46%|████▌     | 228/497 [01:45<02:04,  2.16it/s]\u001b[A\n",
      " 46%|████▌     | 229/497 [01:45<02:03,  2.16it/s]\u001b[A\n",
      " 46%|████▋     | 230/497 [01:45<02:03,  2.16it/s]\u001b[A\n",
      " 46%|████▋     | 231/497 [01:46<02:03,  2.16it/s]\u001b[A\n",
      " 47%|████▋     | 232/497 [01:46<02:02,  2.16it/s]\u001b[A\n",
      " 47%|████▋     | 233/497 [01:47<02:01,  2.17it/s]\u001b[A\n",
      " 47%|████▋     | 234/497 [01:47<02:01,  2.17it/s]\u001b[A\n",
      " 47%|████▋     | 235/497 [01:48<02:01,  2.16it/s]\u001b[A\n",
      " 47%|████▋     | 236/497 [01:48<02:00,  2.17it/s]\u001b[A\n",
      " 48%|████▊     | 237/497 [01:49<01:59,  2.17it/s]\u001b[A\n",
      " 48%|████▊     | 238/497 [01:49<01:59,  2.17it/s]\u001b[A\n",
      " 48%|████▊     | 239/497 [01:50<01:59,  2.17it/s]\u001b[A\n",
      " 48%|████▊     | 240/497 [01:50<01:58,  2.17it/s]\u001b[A\n",
      " 48%|████▊     | 241/497 [01:51<01:57,  2.18it/s]\u001b[A\n",
      " 49%|████▊     | 242/497 [01:51<01:57,  2.17it/s]\u001b[A\n",
      " 49%|████▉     | 243/497 [01:51<01:56,  2.17it/s]\u001b[A\n",
      " 49%|████▉     | 244/497 [01:52<01:56,  2.17it/s]\u001b[A\n",
      " 49%|████▉     | 245/497 [01:52<01:55,  2.17it/s]\u001b[A\n",
      " 49%|████▉     | 246/497 [01:53<01:55,  2.17it/s]\u001b[A\n",
      " 50%|████▉     | 247/497 [01:53<01:54,  2.18it/s]\u001b[A\n",
      " 50%|████▉     | 248/497 [01:54<01:54,  2.18it/s]\u001b[A\n",
      " 50%|█████     | 249/497 [01:54<01:53,  2.18it/s]\u001b[A\n",
      " 50%|█████     | 250/497 [01:55<01:53,  2.17it/s]\u001b[A\n",
      " 51%|█████     | 251/497 [01:55<01:53,  2.17it/s]\u001b[A\n",
      " 51%|█████     | 252/497 [01:56<01:52,  2.17it/s]\u001b[A\n",
      " 51%|█████     | 253/497 [01:56<01:52,  2.18it/s]\u001b[A\n",
      " 51%|█████     | 254/497 [01:57<01:51,  2.17it/s]\u001b[A\n",
      " 51%|█████▏    | 255/497 [01:57<01:51,  2.17it/s]\u001b[A\n",
      " 52%|█████▏    | 256/497 [01:57<01:51,  2.17it/s]\u001b[A\n",
      " 52%|█████▏    | 257/497 [01:58<01:50,  2.17it/s]\u001b[A\n",
      " 52%|█████▏    | 258/497 [01:58<01:50,  2.17it/s]\u001b[A\n",
      " 52%|█████▏    | 259/497 [01:59<01:49,  2.17it/s]\u001b[A\n",
      " 52%|█████▏    | 260/497 [01:59<01:49,  2.17it/s]\u001b[A\n",
      " 53%|█████▎    | 261/497 [02:00<01:48,  2.17it/s]\u001b[A\n",
      " 53%|█████▎    | 262/497 [02:00<01:48,  2.17it/s]\u001b[A\n",
      " 53%|█████▎    | 263/497 [02:01<01:47,  2.17it/s]\u001b[A\n",
      " 53%|█████▎    | 264/497 [02:01<01:47,  2.17it/s]\u001b[A\n",
      " 53%|█████▎    | 265/497 [02:02<01:46,  2.17it/s]\u001b[A\n",
      " 54%|█████▎    | 266/497 [02:02<01:46,  2.17it/s]\u001b[A\n",
      " 54%|█████▎    | 267/497 [02:03<01:45,  2.17it/s]\u001b[A\n",
      " 54%|█████▍    | 268/497 [02:03<01:45,  2.18it/s]\u001b[A\n",
      " 54%|█████▍    | 269/497 [02:03<01:44,  2.18it/s]\u001b[A\n",
      " 54%|█████▍    | 270/497 [02:04<01:44,  2.17it/s]\u001b[A\n",
      " 55%|█████▍    | 271/497 [02:04<01:44,  2.17it/s]\u001b[A\n",
      " 55%|█████▍    | 272/497 [02:05<01:43,  2.17it/s]\u001b[A\n",
      " 55%|█████▍    | 273/497 [02:05<01:43,  2.17it/s]\u001b[A\n",
      " 55%|█████▌    | 274/497 [02:06<01:42,  2.17it/s]\u001b[A\n",
      " 55%|█████▌    | 275/497 [02:06<01:42,  2.17it/s]\u001b[A\n",
      " 56%|█████▌    | 276/497 [02:07<01:41,  2.17it/s]\u001b[A\n",
      " 56%|█████▌    | 277/497 [02:07<01:41,  2.17it/s]\u001b[A\n",
      " 56%|█████▌    | 278/497 [02:08<01:40,  2.17it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "save_every = 1\n",
    "my_model.to(DEVICE)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(my_model.parameters(), \n",
    "#                               lr = 1e-5,\n",
    "#                               eps=1e-8\n",
    "#                               )\n",
    "# num_train_optimization_steps = num_epoch * len(train_dl)\n",
    "# scheduler = transformers.get_linear_schedule_with_warmup(optimizer,\n",
    "#                                                          num_warmup_steps=100, num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "param_optimizer = list(my_model.named_parameters())\n",
    "weight_decay = 0.01\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, \n",
    "                              lr = 1e-5,\n",
    "                              eps=1e-8\n",
    "                              )\n",
    "num_train_optimization_steps = num_epoch * len(train_dl)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer,\n",
    "                                                         num_warmup_steps=100, num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    with torch.autograd.set_detect_anomaly(False):\n",
    "        for epoch in tqdm.auto.tqdm(range(num_epoch)):\n",
    "            print(\"-\" * 10, epoch + 1)\n",
    "            avg_loss = 0\n",
    "            my_model.train()\n",
    "            s1 = []\n",
    "            s2 = []\n",
    "            for i, b in enumerate(tqdm.auto.tqdm(train_dl)):\n",
    "                i1, m1, i2, m2, s = b\n",
    "                #print(i1.shape, i2.shape, s.shape)\n",
    "                s = s.to(torch.float)\n",
    "                optimizer.zero_grad()\n",
    "                o1 = my_model(i1.to(DEVICE), m1.to(DEVICE))\n",
    "                o2 = my_model(i2.to(DEVICE), m2.to(DEVICE))\n",
    "                #print(o1.shape, o2.shape, s)\n",
    "                loss = crition([o1,o2], s.to(DEVICE))\n",
    "                # print(f\"loss = {loss.item()}, epoch={epoch}\")\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(my_model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                avg_loss += loss.item()\n",
    "                score = nn.functional.cosine_similarity(o1, o2)\n",
    "                s1.append(score.cpu().detach().numpy())\n",
    "                s2.append(s.cpu().detach().numpy())\n",
    "                if np.isnan(loss.item()):\n",
    "                    raise Exception(\"Found Nan\")\n",
    "                #break\n",
    "                if (i + 1) % 30 == 0:\n",
    "                    print(f\"loss = {loss.item()}, epoch={epoch}\")\n",
    "            x = np.hstack(s1)\n",
    "            y = np.hstack(s2)\n",
    "            p = stats.spearmanr(x,y)\n",
    "            print(f\"avg_train_loss = {avg_loss/len(train_dl)}, p= {p}\")\n",
    "            test(my_model, test_dl, crition, \"test\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a878e697-83d1-4936-8d6e-5254b3958c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test model  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 497/497 [03:38<00:00,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 0.09914359622111067, p= SignificanceResult(statistic=0.4550205466982067, pvalue=5.011819138005421e-102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(my_model, test_dl, crition, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43c531-bf2c-44aa-ba4d-9e5bb84a4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out1 = my_model(i2.to(DEVICE),m2.to(DEVICE))\n",
    "# out2 = my_model(i1.to(DEVICE), m1.to(DEVICE))\n",
    "# model(i2, m2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
